{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa05f3f8",
      "metadata": {
        "id": "aa05f3f8"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lopesmatheus/COA19-PythonSimu/blob/main/Main_PIE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b4cdcb57",
      "metadata": {
        "id": "b4cdcb57"
      },
      "source": [
        "# PIE - Apprentissage par renforcement pour le contrôle optimal des systèmes d’air - Notebook 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4ded3eb2",
      "metadata": {},
      "source": [
        "Ce notebook a pour objet de répondre au besoin exprimé par Liebherr Aerospace et qui s'inscrit dans le PIE-COA19 \"Apprentissage par renforcement pour le contrôle optimal des systèmes d'air\". Ce deuxième notebook applique l'algorithme construit sur le cas réel."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "be8d29ae",
      "metadata": {
        "id": "be8d29ae"
      },
      "source": [
        "## Prérequis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c6c0c5d2",
      "metadata": {},
      "source": [
        "### À jouer sous Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40s6gnrBlDtn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40s6gnrBlDtn",
        "outputId": "f47b55d5-ff49-4844-abee-180f7b845fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.15.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e495d7b",
      "metadata": {
        "id": "6e495d7b",
        "outputId": "545b199d-b59d-4b07-846c-3ee957e831b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from torch) (3.10.0.2)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b86e247",
      "metadata": {
        "id": "5b86e247",
        "outputId": "52d1753f-89a8-4d9f-bb89-4f29ce3b2427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in c:\\users\\mathe\\anaconda3\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from gym) (4.8.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from gym) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from gym) (1.20.3)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\mathe\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "38X8ZX2_GIra",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38X8ZX2_GIra",
        "outputId": "3035de1a-1436-4b13-e19a-6313c0c974fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fmpy\n",
            "  Downloading FMPy-0.3.15-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from fmpy) (2022.7.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from fmpy) (4.9.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from fmpy) (22.2.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from fmpy) (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fmpy) (1.22.4)\n",
            "Collecting lark\n",
            "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.8/dist-packages (from fmpy) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2->fmpy) (2.1.2)\n",
            "Installing collected packages: lark, fmpy\n",
            "Successfully installed fmpy-0.3.15 lark-1.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install fmpy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9f49d553",
      "metadata": {},
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a91ec20",
      "metadata": {
        "id": "8a91ec20"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "import shutil\n",
        "import pandas as pd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "384aa912",
      "metadata": {
        "id": "384aa912"
      },
      "source": [
        "# Environnement PLANT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33fa3663",
      "metadata": {
        "id": "33fa3663"
      },
      "outputs": [],
      "source": [
        "from fmpy import read_model_description, extract\n",
        "from fmpy.fmi2 import FMU2Slave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "23f392cd",
      "metadata": {
        "id": "23f392cd"
      },
      "outputs": [],
      "source": [
        "# define the model name and simulation parameters\n",
        "\n",
        "class PlantEnv:\n",
        "    def __init__(self,CaseTest,Hysteresis,step_size):\n",
        "        \n",
        "        self.Hysteresis = Hysteresis     # Définit l'hystérésis de la vanne 0 -> 0%, 1 -> 50%, 2 -> 100%\n",
        "        \n",
        "        self.CaseTest = CaseTest         # Reçoit la classe du cas test qui sera simulé (plusieurs ont été codé et\n",
        "                                         # et ont la même structure pour la simulation)\n",
        "        \n",
        "        fmu_plant_filename = '../fmus/PLANT_v3_mac.fmu'   # Nom de fichier fmu de la PLANT. Il faudra adapter au cas vous avez changé le\n",
        "                                              # nom ou le dossier, ou en fonction de votre O.S.\n",
        "            \n",
        "        self.step_size = step_size       # Pas de temps de la simulation\n",
        "        \n",
        "        self.Tend = 2000#self.CaseTest.getTsimu() - self.CaseTest.getTend()  # Defini la duree de la simulation\n",
        "\n",
        "        self.action_space = gym.spaces.Box(0,200,(1,),np.float64)\n",
        "\n",
        "        self.observation_space=gym.spaces.Box(np.array([-100.0 for i in range(4)]),np.array([300.0 for i in range(4)]),(4,),np.float64)\n",
        "\n",
        "\n",
        "        # read the model description\n",
        "        plant_description = read_model_description(fmu_plant_filename)   # Reçoit les paramètres du modèle \n",
        "\n",
        "        # collect the value references\n",
        "        self.plant_vrs = {}                                              # Liste qui reçoit les valeurs de référence de la\n",
        "                                                                         # plante. Chaque référence identifie soit une entrée \n",
        "                                                                         # soit une sortie de la plante.\n",
        "        for variable in plant_description.modelVariables:\n",
        "            self.plant_vrs[variable.name] = variable.valueReference\n",
        "\n",
        "        self.inputs_plant = [v for v in plant_description.modelVariables if v.causality == 'input']  # Recupere tous les valeurs\n",
        "                                                                                                     # de reference identifiées\n",
        "                                                                                                     # comme input\n",
        "                \n",
        "        self.outputs_plant = [v for v in plant_description.modelVariables if v.causality == 'output'] # Recupere tous les valeurs\n",
        "                                                                                                      # de reference identifiées\n",
        "                                                                                                      # comme output\n",
        "\n",
        "        input_plant_order = []\n",
        "        \n",
        "        for v in self.inputs_plant:\n",
        "            input_plant_order.append(v.valueReference)\n",
        "\n",
        "        self.CaseTest.setInputOrder(input_plant_order)  # Passe une liste qui définit l'ordre des entrées dans la plant\n",
        "\n",
        "        # extract the FMU\n",
        "        # Ces prochaines lignes s'agissent de l'extraction de la fmu (ouverture du fichier) et initialisation comme indiqué dans\n",
        "        # les exemples de documentation de la librarie fmipy \n",
        "        # https://github.com/CATIA-Systems/FMPy/blob/main/fmpy/examples/parameter_variation.py\n",
        "        \n",
        "        self.unzipdir_plant = extract(fmu_plant_filename)\n",
        "\n",
        "        self.fmu_plant = FMU2Slave(guid=plant_description.guid,\n",
        "                             unzipDirectory=self.unzipdir_plant,\n",
        "                             modelIdentifier=plant_description.coSimulation.modelIdentifier,\n",
        "                             instanceName='instance1')\n",
        "\n",
        "        # initialize\n",
        "        self.fmu_plant.instantiate()\n",
        "        self.fmu_plant.setupExperiment(startTime=0)\n",
        "        self.fmu_plant.enterInitializationMode()\n",
        "        self.fmu_plant.exitInitializationMode()\n",
        "        \n",
        "        self.fmu_plant.setReal([self.inputs_plant[1].valueReference], [self.Hysteresis])   # Définit l'entrée constante de\n",
        "                                                                                           # l'hysteresis\n",
        "        \n",
        "        for cst in self.CaseTest.getConstantsInputs():                  # Ce boucle va reprendre les entrées qui sont definies\n",
        "            vr = self.inputs_plant[cst].valueReference                  # comme constants au cours de toute la simulation et qui    \n",
        "            self.fmu_plant.setReal([vr], [self.CaseTest.getAll(vr,0)])  # non pas besoin d'être redefinies à chaque iteration\n",
        "                                                                        # (gain de cout computationelle)\n",
        "            \n",
        "        self.T_target = self.CaseTest.getT_tgt_C()                      # Cela prend le vecteur de temperature target\n",
        "            \n",
        "        self.counter = 0   # Initialise le counteur\n",
        "        self.time = 0      # Initialise le temps de simulation\n",
        "\n",
        "        self.state = np.zeros((self.observation_space.shape[0],)) # Vecteur pour sauvegarder les états d'avant\n",
        "        \n",
        "    def reset(self):\n",
        "        \n",
        "        # Cette function a pour objectif de faire le \"reset\" de la plante à fin d'initialiser une nouvelle simulation\n",
        "        \n",
        "        self.fmu_plant.reset()\n",
        "        self.fmu_plant.setupExperiment(startTime=0)    # Initialisation de la nouvelle simulation\n",
        "        self.fmu_plant.enterInitializationMode()\n",
        "        self.fmu_plant.exitInitializationMode()\n",
        "        \n",
        "        # On doit, à nouveau, definir les entrées qui sont constantes avant le début de la simulation\n",
        "        \n",
        "        self.fmu_plant.setReal([self.inputs_plant[1].valueReference], [self.Hysteresis])    \n",
        "        \n",
        "        for cst in self.CaseTest.getConstantsInputs():\n",
        "            vr = self.inputs_plant[cst].valueReference\n",
        "            self.fmu_plant.setReal([vr], [self.CaseTest.getAll(vr,0)])\n",
        "            \n",
        "        self.T_target = self.CaseTest.getT_tgt_C()\n",
        "            \n",
        "        self.counter = 0\n",
        "        self.time = 0\n",
        "\n",
        "        # Comme pendant les 550 premieres pas de temps on ne peut pas commender la vanne\n",
        "        # de manière à annuler l'erreur, on realise la simulation sans commande pour arriver\n",
        "        # à un point où les données sont plus utiles à l'entrainement\n",
        "\n",
        "        init_state=np.zeros((self.observation_space.shape[0],))\n",
        "\n",
        "        for i in range(550):\n",
        "          state, reward, done, _ = self.step(0)\n",
        "          if i>=546:\n",
        "            init_state[i-546]=state[0]\n",
        "\n",
        "        self.state = init_state\n",
        "\n",
        "        #print(self.state)\n",
        "\n",
        "        return(init_state)\n",
        "        \n",
        "    def step(self, action):\n",
        "        for var in self.CaseTest.getVariablesInputs():             # Define tous les variables qui change à chaque pas de temps\n",
        "            vr = self.inputs_plant[var].valueReference             # en fonction de ce qui est code dans le cas test\n",
        "            self.fmu_plant.setReal([vr], [self.CaseTest.getAll(vr,self.counter)])\n",
        "            \n",
        "        self.fmu_plant.setReal([self.inputs_plant[0].valueReference], [action])    # Passe l'action qui sera faite à la plante\n",
        "        \n",
        "        self.fmu_plant.doStep(currentCommunicationPoint=self.time, communicationStepSize=self.step_size) # Simule 1 pas de temps\n",
        "                                                                                                         # de la plante\n",
        "        \n",
        "        self.counter += 1\n",
        "        self.time += self.step_size\n",
        "        \n",
        "        if self.time < self.Tend:            # Identifie si la simulation est terminée\n",
        "            done = False\n",
        "        else:\n",
        "            done = True\n",
        "        \n",
        "        TBAS_SENSOR = self.fmu_plant.getReal([self.outputs_plant[4].valueReference])[0]    # Prend la temperature de sortie\n",
        "                                                                                           # de la plante\n",
        "        \n",
        "        T_target = self.CaseTest.getT_tgt_C()\n",
        "        \n",
        "        error = abs(TBAS_SENSOR - T_target[0])\n",
        "        \n",
        "        #reward = - error**2 #- 100 * max(self.time-60,0) * max(error-10,0) - 100 * max(self.time-120,0) * max(error-5,0)    \n",
        "        reward = - (error / T_target[0])**2\n",
        "        \n",
        "        self.state[0:len(self.state)-1] = self.state[1:len(self.state)]   # Actualise les états d'avant qui l'on garde\n",
        "        self.state[len(self.state)-1] = TBAS_SENSOR\n",
        "            \n",
        "\n",
        "        return self.state, reward, done, _\n",
        "    \n",
        "    def kill_plant(self):                   # Après la simulation, on peut \"tuer\" la plante et la \"déinstancier\"\n",
        "        self.fmu_plant.terminate()\n",
        "        self.fmu_plant.freeInstance()\n",
        "\n",
        "        # clean up\n",
        "        shutil.rmtree(self.unzipdir_plant, ignore_errors=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bb278307",
      "metadata": {
        "id": "bb278307"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "05c7b4cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05c7b4cd",
        "outputId": "3d373d73-3606-45fd-ca6b-10ff64a705b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, device=DEVICE):\n",
        "        self.capacity = capacity # capacity of the buffer\n",
        "        self.device = device\n",
        "        self.data = []\n",
        "        self.index = 0 # index of the next cell to be filled\n",
        "\n",
        "    def append(self, s, a, r, s_, d):\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(None)\n",
        "        self.data[self.index] = (s, a, r, s_, d)\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.data, batch_size)\n",
        "        return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    An global agent class that describe the interactions between our agent and it's environment\n",
        "    \"\"\"\n",
        "    def __init__(self, state_space, action_space, device=DEVICE, name=\"base_agent\"):\n",
        "\n",
        "        self.name = name  # The name is used inside plot legend, outputs directory path, and outputs file names\n",
        "\n",
        "        self.state_space = state_space\n",
        "        self.state_shape = state_space.shape\n",
        "        self.state_size = state_space.shape[0]  # Assume state space is continuous\n",
        "\n",
        "        self.continuous = isinstance(action_space, gym.spaces.Box)\n",
        "        self.action_space = action_space\n",
        "        self.nb_actions = self.action_space.shape[0] if self.continuous else self.action_space.n\n",
        "        print(self.nb_actions)\n",
        "        self.last_state = None  # Useful to store interaction when we recieve (new_stare, reward, done) tuple\n",
        "        self.device = device\n",
        "        self.episode_id = 0\n",
        "        self.episode_time_step_id = 0\n",
        "        self.time_step_id = 0\n",
        "    \n",
        "    def on_simulation_start(self):\n",
        "        \"\"\"\n",
        "        Called when an episode is started. will be used by child class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def on_episode_start(self, state):\n",
        "        self.last_state = state\n",
        "        self.episode_time_step_id = 0\n",
        "        self.episode_id = 0\n",
        "\n",
        "    def action(self, state):\n",
        "        res = self.action_space.sample()\n",
        "        return res\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.episode_time_step_id += 1\n",
        "        self.time_step_id += 1\n",
        "        self.last_state = new_state\n",
        "\n",
        "    def on_episode_stop(self):\n",
        "        self.episode_id += 1\n",
        "\n",
        "    def on_simulation_stop(self):\n",
        "        pass\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "76b47bdf",
      "metadata": {
        "id": "76b47bdf"
      },
      "source": [
        "## Actor-critic\n",
        "<div id=\"actor_critic\"></div>\n",
        "\n",
        "### Deep-Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "<div id=\"ddpg\"></div>\n",
        "\n",
        "On reprend l'algorithme de DDPG développé dans le .ipynb du cas jouet (Main_PIE_Base_case_final)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce34aab0",
      "metadata": {
        "id": "ce34aab0"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "def init_weights(layer, bound=None):\n",
        "    if bound is None:\n",
        "        bound = 1. / np.sqrt(layer.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(layer.weight.data, -bound, bound)\n",
        "    torch.nn.init.uniform_(layer.bias.data, -bound, bound)\n",
        "\n",
        "\n",
        "class DefaultNNActor(nn.Module):\n",
        "    def __init__(self, learning_rate, input_dims, layer_1_dims, layer_2_dims, output_dims, device,\n",
        "                 last_activation=None):\n",
        "        super().__init__()\n",
        "        self.last_activation = last_activation\n",
        "        self.layer_1 = nn.Linear(input_dims, layer_1_dims)\n",
        "        init_weights(self.layer_1)\n",
        "        self.layer_norm_1 = nn.LayerNorm(layer_1_dims)\n",
        "\n",
        "        self.layer_2 = nn.Linear(layer_1_dims, layer_2_dims)\n",
        "        init_weights(self.layer_2)\n",
        "        self.layer_norm_2 = nn.LayerNorm(layer_2_dims)\n",
        "\n",
        "        self.layer_3 = nn.Linear(layer_2_dims, output_dims)\n",
        "        init_weights(self.layer_3, bound=0.003)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        running_output = self.layer_1(inputs)\n",
        "        running_output = self.layer_norm_1(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_2(running_output)\n",
        "        running_output = self.layer_norm_2(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_3(running_output)\n",
        "        running_output = torch.tanh(running_output)\n",
        "\n",
        "        running_output = 152.5 + 27.5 * running_output\n",
        "\n",
        "        return running_output\n",
        "\n",
        "    def converge_to(self, other_model, tau=0.01):\n",
        "        \"\"\"\n",
        "        Make the value of parameters of this model converge to one from the given model.\n",
        "        The parameter tau indicate how close our weights should be from the one of the other model.\n",
        "        self.converge_to(other_model, tau=1) is equivalent to self = copy.deepcopy(other_model).\n",
        "\n",
        "        other_model should have the same shape, dimensions, than self.\n",
        "        \"\"\"\n",
        "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
        "            self_param.data.copy_(\n",
        "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
        "            )\n",
        "\n",
        "class DefaultNNCritic(nn.Module):\n",
        "    def __init__(self, learning_rate, input_dims, layer_1_dims, layer_2_dims, output_dims, device,\n",
        "                 last_activation=None):\n",
        "        super().__init__()\n",
        "        self.last_activation = last_activation\n",
        "        self.layer_1 = nn.Linear(input_dims, layer_1_dims)\n",
        "        init_weights(self.layer_1)\n",
        "        self.layer_norm_1 = nn.LayerNorm(layer_1_dims)\n",
        "\n",
        "        self.layer_2 = nn.Linear(layer_1_dims, layer_2_dims)\n",
        "        init_weights(self.layer_2)\n",
        "        self.layer_norm_2 = nn.LayerNorm(layer_2_dims)\n",
        "\n",
        "        self.layer_3 = nn.Linear(layer_2_dims, output_dims)\n",
        "        init_weights(self.layer_3, bound=0.003)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        running_output = self.layer_1(inputs)\n",
        "        running_output = self.layer_norm_1(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_2(running_output)\n",
        "        running_output = self.layer_norm_2(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_3(running_output)\n",
        "\n",
        "        if self.last_activation is not None:\n",
        "            running_output = self.last_activation(running_output)\n",
        "        return running_output\n",
        "\n",
        "    def converge_to(self, other_model, tau=0.01):\n",
        "        \"\"\"\n",
        "        Make the value of parameters of this model converge to one from the given model.\n",
        "        The parameter tau indicate how close our weights should be from the one of the other model.\n",
        "        self.converge_to(other_model, tau=1) is equivalent to self = copy.deepcopy(other_model).\n",
        "\n",
        "        other_model should have the same shape, dimensions, than self.\n",
        "        \"\"\"\n",
        "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
        "            self_param.data.copy_(\n",
        "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d6RjX_UNQj_y",
      "metadata": {
        "id": "d6RjX_UNQj_y"
      },
      "outputs": [],
      "source": [
        "def simulation(environment, agent, nb_episodes=200, verbose=True):\n",
        "    episodes_rewards_sum = []\n",
        "    agent.on_simulation_start()\n",
        "    for episode_id in range(nb_episodes):\n",
        "        state = environment.reset()\n",
        "        agent.on_episode_start(state)\n",
        "\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        prev=np.zeros((1,))\n",
        "\n",
        "        x=[]\n",
        "        actions=[]\n",
        "        while not done:\n",
        "            action = agent.action(state)\n",
        "            actions.append(action)\n",
        "            x.append(state[-1])\n",
        "            state, reward, done ,_= environment.step(action)\n",
        "            \n",
        "            agent.on_action_stop(action, state, reward, done)\n",
        "\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        agent.on_episode_stop()\n",
        "        rewards_sum = sum(episode_rewards)\n",
        "        episodes_rewards_sum.append(rewards_sum)\n",
        "        #environment.close()\n",
        "        if len(episodes_rewards_sum) > 20:\n",
        "            last_20_average = mean(episodes_rewards_sum[-20:])\n",
        "        else:\n",
        "            last_20_average = mean(episodes_rewards_sum)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Episode \", '{:3d}'.format(episode_id),\n",
        "                  \", episode return \", '{:4.1f}'.format(rewards_sum),\n",
        "                  \", last 20 avg \", '{:4.1f}'.format(last_20_average),\n",
        "                  sep='')\n",
        "            t=range(int(1.0/float(step_size)*1450))\n",
        "            plt.figure()\n",
        "            plt.plot(t,x)\n",
        "            plt.plot(t,[CaseTest.getT_tgt_C() for i in range(1450)])\n",
        "            plt.xlabel('t (step)')\n",
        "            plt.ylabel('x')\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(t,actions)\n",
        "\n",
        "            plt.show()\n",
        "    return episodes_rewards_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e89db5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9e89db5f",
        "outputId": "d423cbb7-95ed-44e2-a96a-5138bf47e206"
      },
      "outputs": [],
      "source": [
        "class DDPGAgent(Agent):\n",
        "    def __init__(self, state_space, action_space, device, actor_lr=0.000005, critic_lr=0.00001, tau=0.001, gamma=0.99,\n",
        "                 max_size=100000, layer1_size=512, layer2_size=256, batch_size=64, noise_std=0.1, name=\"DDPG\"):\n",
        "        assert isinstance(action_space, gym.spaces.Box)\n",
        "        super().__init__(state_space, action_space, device=device, name=name)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.replay_buffer = ReplayBuffer(max_size, self.device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.actor = DefaultNNActor(actor_lr, self.state_size, layer1_size, layer2_size, self.nb_actions\n",
        "                               , self.device)\n",
        "        self.critic = DefaultNNCritic(critic_lr, self.state_size + self.nb_actions, layer1_size, layer2_size, 1, self.device)\n",
        "\n",
        "        print(self.actor)\n",
        "\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.normal_distribution = torch.distributions.normal.Normal(\n",
        "            torch.zeros(self.nb_actions), torch.full((self.nb_actions,), noise_std))\n",
        "\n",
        "    def action(self, observation):\n",
        "        with torch.no_grad():\n",
        "            observation = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
        "            actor_output = self.actor.forward(observation).to(self.device)\n",
        "            #action=torch.normal(actor_output[0], np.abs(actor_output[1]))\n",
        "            noise = self.normal_distribution.sample().to(self.device)\n",
        "            action = actor_output + noise\n",
        "        return action.cpu().detach().numpy()\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
        "        self.learn()\n",
        "        super().on_action_stop(action, new_state, reward, done)\n",
        "\n",
        "    def learn(self):\n",
        "        #if len(self.replay_buffer) > self.batch_size:\n",
        "        if len(self.replay_buffer.data) > self.batch_size:\n",
        "            states, actions, rewards, new_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target_actions = self.target_actor.forward(new_states)\n",
        "                critic_value_ = self.target_critic.forward(torch.concat((new_states, target_actions), dim=-1))\n",
        "            critic_value = self.critic.forward(torch.concat((states, actions), dim=-1))\n",
        "            target = torch.addcmul(rewards, self.gamma, 1 - dones, critic_value_.squeeze()).view(self.batch_size, 1)\n",
        "            self.critic.optimizer.zero_grad()\n",
        "            critic_loss = torch.nn.functional.mse_loss(target, critic_value)\n",
        "            critic_loss.backward()\n",
        "            self.critic.optimizer.step()\n",
        "\n",
        "            self.actor.optimizer.zero_grad()\n",
        "            actions = self.actor.forward(states)\n",
        "            actor_loss = - self.critic.forward(torch.concat((states, actions), dim=-1))\n",
        "            actor_loss = torch.mean(actor_loss)\n",
        "            actor_loss.backward()\n",
        "            self.actor.optimizer.step()\n",
        "\n",
        "            self.target_critic.converge_to(self.critic, tau=self.tau)\n",
        "            self.target_actor.converge_to(self.actor, tau=self.tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8b274e24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../')\n",
        "from Tests.testcases.A_T01a import A_T01a ##selectionner ici le cas test à utiliser."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c92774af",
      "metadata": {},
      "source": [
        "On exécute alors la simulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9abbfb64",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "DefaultNNActor(\n",
            "  (layer_1): Linear(in_features=4, out_features=512, bias=True)\n",
            "  (layer_norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (layer_3): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ww/cj0nc7j53fl0jllbhcvfj8s80000gn/T/ipykernel_5883/3197761815.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
            "  return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
            "/var/folders/ww/cj0nc7j53fl0jllbhcvfj8s80000gn/T/ipykernel_5883/161924435.py:46: UserWarning: This overload of addcmul is deprecated:\n",
            "\taddcmul(Tensor input, Number value, Tensor tensor1, Tensor tensor2, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddcmul(Tensor input, Tensor tensor1, Tensor tensor2, *, Number value, Tensor out) (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  target = torch.addcmul(rewards, self.gamma, 1 - dones, critic_value_.squeeze()).view(self.batch_size, 1)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m environment \u001b[39m=\u001b[39m PlantEnv(CaseTest,\u001b[39m0.0\u001b[39m,step_size)\n\u001b[1;32m      5\u001b[0m agent \u001b[39m=\u001b[39m DDPGAgent(environment\u001b[39m.\u001b[39mobservation_space, environment\u001b[39m.\u001b[39maction_space, noise_std\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39mDEVICE)\n\u001b[0;32m----> 6\u001b[0m simulation(environment, agent,nb_episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn [7], line 20\u001b[0m, in \u001b[0;36msimulation\u001b[0;34m(environment, agent, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m     x\u001b[39m.\u001b[39mappend(state[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     18\u001b[0m     state, reward, done ,_\u001b[39m=\u001b[39m environment\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m---> 20\u001b[0m     agent\u001b[39m.\u001b[39;49mon_action_stop(action, state, reward, done)\n\u001b[1;32m     22\u001b[0m     episode_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     24\u001b[0m agent\u001b[39m.\u001b[39mon_episode_stop()\n",
            "Cell \u001b[0;32mIn [9], line 34\u001b[0m, in \u001b[0;36mDDPGAgent.on_action_stop\u001b[0;34m(self, action, new_state, reward, done)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_action_stop\u001b[39m(\u001b[39mself\u001b[39m, action, new_state, reward, done):\n\u001b[1;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_state, action, reward, new_state, done)\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n\u001b[1;32m     35\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mon_action_stop(action, new_state, reward, done)\n",
            "Cell \u001b[0;32mIn [9], line 60\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_critic\u001b[39m.\u001b[39mconverge_to(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic, tau\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau)\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_actor\u001b[39m.\u001b[39;49mconverge_to(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor, tau\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtau)\n",
            "Cell \u001b[0;32mIn [6], line 53\u001b[0m, in \u001b[0;36mDefaultNNActor.converge_to\u001b[0;34m(self, other_model, tau)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mMake the value of parameters of this model converge to one from the given model.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mThe parameter tau indicate how close our weights should be from the one of the other model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mother_model should have the same shape, dimensions, than self.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m self_param, other_param \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters(), other_model\u001b[39m.\u001b[39mparameters()):\n\u001b[0;32m---> 53\u001b[0m     self_param\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcopy_(\n\u001b[1;32m     54\u001b[0m         self_param\u001b[39m.\u001b[39;49mdata \u001b[39m*\u001b[39;49m (\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m tau) \u001b[39m+\u001b[39;49m other_param\u001b[39m.\u001b[39;49mdata \u001b[39m*\u001b[39;49m tau\n\u001b[1;32m     55\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "step_size = 1\n",
        "CaseTest = A_T01a(step_size)  \n",
        "environment = PlantEnv(CaseTest,0.0,step_size)\n",
        "\n",
        "agent = DDPGAgent(environment.observation_space, environment.action_space, noise_std=2, device=DEVICE)\n",
        "simulation(environment, agent,nb_episodes=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f_Lf95wigovM",
      "metadata": {
        "id": "f_Lf95wigovM"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.actor.state_dict(), 'act_checkpoint.pth')\n",
        "#agent.critic.save('act_checkpoint.pth')\n",
        "torch.save(agent.critic.state_dict(), 'crit_checkpoint.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
