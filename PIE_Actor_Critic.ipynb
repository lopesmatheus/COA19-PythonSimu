{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lopesmatheus/COA19-PythonSimu/blob/main/PIE_Actor_Critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4cdcb57",
      "metadata": {
        "id": "b4cdcb57"
      },
      "source": [
        "# PIE - DDPG Continuous"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be8d29ae",
      "metadata": {
        "id": "be8d29ae"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40s6gnrBlDtn",
        "outputId": "386933a4-7d2c-4ded-b99c-99b72aae7ddf"
      },
      "id": "40s6gnrBlDtn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.12.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8a91ec20",
      "metadata": {
        "id": "8a91ec20"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb278307",
      "metadata": {
        "id": "bb278307"
      },
      "source": [
        "### Agents\n",
        "\n",
        "To simplify the global code of this notebook, I propose you tho use a mother class for each of our agents. This will allow us to build a simulation function that take in argument an environment and an agent, and work with any agents and any environments. Then all you will have to do is to describe what the agent should do at each step of the learning.\n",
        "\n",
        "I give you here this class, with the replay buffer from RL5 notebook, and an implementation of DQN that follow ou main Agent structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "05c7b4cd",
      "metadata": {
        "id": "05c7b4cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f998b89-bfb4-47f7-cc06-00d31b152e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity, device=DEVICE):\n",
        "        self.capacity = capacity # capacity of the buffer\n",
        "        self.device = device\n",
        "        self.data = []\n",
        "        self.index = 0 # index of the next cell to be filled\n",
        "\n",
        "    def append(self, s, a, r, s_, d):\n",
        "        if len(self.data) < self.capacity:\n",
        "            self.data.append(None)\n",
        "        self.data[self.index] = (s, a, r, s_, d)\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.data, batch_size)\n",
        "        return list(map(lambda x:torch.Tensor(x).to(self.device), list(zip(*batch))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    An global agent class that describe the interactions between our agent and it's environment\n",
        "    \"\"\"\n",
        "    def __init__(self, state_space, action_space, device=DEVICE, name=\"base_agent\"):\n",
        "\n",
        "        self.name = name  # The name is used inside plot legend, outputs directory path, and outputs file names\n",
        "\n",
        "        self.state_space = state_space\n",
        "        self.state_shape = state_space.shape\n",
        "        self.state_size = state_space.shape[0]  # Assume state space is continuous\n",
        "\n",
        "        self.continuous = isinstance(action_space, gym.spaces.Box)\n",
        "        self.action_space = action_space\n",
        "        self.nb_actions = self.action_space.shape[0] if self.continuous else self.action_space.n\n",
        "        self.last_state = None  # Useful to store interaction when we recieve (new_stare, reward, done) tuple\n",
        "        self.device = device\n",
        "        self.episode_id = 0\n",
        "        self.episode_time_step_id = 0\n",
        "        self.time_step_id = 0\n",
        "    \n",
        "    def on_simulation_start(self):\n",
        "        \"\"\"\n",
        "        Called when an episode is started. will be used by child class.\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def on_episode_start(self, state):\n",
        "        self.last_state = state\n",
        "        self.episode_time_step_id = 0\n",
        "        self.episode_id = 0\n",
        "\n",
        "    def action(self, state):\n",
        "        res = self.action_space.sample()\n",
        "        return res\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.episode_time_step_id += 1\n",
        "        self.time_step_id += 1\n",
        "        self.last_state = new_state\n",
        "\n",
        "    def on_episode_stop(self):\n",
        "        self.episode_id += 1\n",
        "\n",
        "    def on_simulation_stop(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2c4ae9",
      "metadata": {
        "id": "5f2c4ae9"
      },
      "source": [
        "Now let's build a function that run a simulation where a given agent interact with a givent environment. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b47bdf",
      "metadata": {
        "id": "76b47bdf"
      },
      "source": [
        "## Actor-critic\n",
        "\n",
        "<div id=\"actor_critic\"></div>\n",
        "\n",
        "As we saw earlier in this class, Deep Q-Network (DQN) was made for discrete actions spaces. \n",
        "It use a Q-network, that is made to give the expected value of each action in a given state.\n",
        "In continuous action space, this algorithm cannot be used because we cannot allocate one neural-network output for each action. \n",
        "To correct this problem, the main idea is to make the Q-Network return a single outpur, that will be the value of the state action pair, that is given in input. But do this prevent us to choose the action, and that's why we add another neural network called the actor, taking a state as the input, and returning the value of each continuous action to take (1 action between -1 and 1 for pendulum, so 1 neuron at the output, 2 for lunar lander).\n",
        "\n",
        "To train this neural network, we just have to use the Q-Network, to estimate how much the chosen action is good to take in the given state.\n",
        "\n",
        "This is the main idea around actor critic architecture. To understand in detail how it work, YOU are going to implement two of the most known off-policy actor critic architectures, Deep-Deterministic Policy Gradient (DDPG) and Soft Actor-Critic (SAC). \n",
        "\n",
        "As in other notebook, the code of these two algorithms are given. But because the number of questions/task to do is low in this notebook compared to others, I hardly recomend you to code them by yourself first. Try to run them, and if it's not working well, try some improvement or to understand what is not working, before to give up and look the solution. You will have the entire session to implement 2 algorithms, so take your time to make sure you understood them well.\n",
        "\n",
        "### Deep-Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "<div id=\"ddpg\"></div>\n",
        "\n",
        "DDPG is considered as an off-policy algorithm, because the policy used to train the critic is different than the one used to generate samples.\n",
        "\n",
        "For the general idea, when we select an interaction (s, a, r, s', a') to train our critic network, the way a' will be chosen will define the reward propagation (generally called credit assignment) speed. If a' is the optimal action from s', like in DQN, there is a high probability to get an action that lead in interestig and highly valuated states s''. In that case, the high values and the rewards will be propagated to (s, a) value, more frequently than s'' with low value.\n",
        "\n",
        "In SARSA case, the a' actions will be chose following GLIE actor (cf RL1 - RL fundamentals notebook), that will depend on an exploration strategy, so a' can be random sometime. When a' is random, we have a high probability to evaluate Q'(s', a') where action a' is a bad action in state s'. In this case, Q(s, a) will also be learned as being a bad action state pair, even if s' can maybe lead to an interesting reward. We call SARSA an on-policy algorithm. For a purely actor-critic on-policy algorithm, you can also check [A3C](https://arxiv.org/pdf/1602.01783.pdf) but you don't need to understand this algorithm in this notebook.\n",
        "\n",
        "If you didn't got it, you can also check this course from Olivier SIGAUD on youtube : https://youtu.be/hlhzvQnXdAA\n",
        "\n",
        "Now, I want you to implement DDPG algorithm, that use an actor network, a critic network, and two target networks for both actor and critic to respectively compute a' and Q'.\n",
        "\n",
        "Note that the default DDPG algorithm don't use epsilon-greedy, but add a noise on the action to make it explore. The original paper use a OU-noise, but for simplicity, we will generate a gaussian noise N(mean, std) to noise our action.\n",
        "\n",
        "To make sure you know how to implement it, let's answer to some questions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb3d016",
      "metadata": {
        "id": "7cb3d016"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "If we have an actor network A, a critic actor network A' and a state s, how will I compute actions a to build samples when my agent is interacting with it's environment ? How will I compute action a' to compute Q'(s', a') to train my critic network ?\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533d28b6",
      "metadata": {
        "id": "533d28b6"
      },
      "source": [
        "<details class=\"alert alert-danger\">\n",
        "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
        "When the agent is generating training samples while interacting with it's environment, he need to explore it to find new interesting states. In this case, the choosen action will be \n",
        "\n",
        "    a = A(s) + noise\n",
        "    \n",
        "When the agent is generations next_action a' to train critic by computing Q'(s', a'), we don't want him to explore so the generated a' have a higher probability to correspond to a highly valuable Q'(s', a'), so this value have a higher change to be propagated to Q(s, a), speeding up the training.\n",
        "<details/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee5034d",
      "metadata": {
        "id": "fee5034d"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "If the critic is used to evaluate the action a taken by the actor network in a given state s. What will be the loss of the critic network ?\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbab0bd1",
      "metadata": {
        "id": "bbab0bd1"
      },
      "source": [
        "<details class=\"alert alert-danger\">\n",
        "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
        "In this situation, the critic is learning a Q-value, like the DQN model is doing. For this reason, the loss of our critic network should be similar:\n",
        "    \n",
        "    # Pseudo code\n",
        "    target = reward + gamma * (1 - done) * target_critic(s', a')\n",
        "    critic_loss = MSE(critic(s, a), target)\n",
        "<details/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93494b2f",
      "metadata": {
        "id": "93494b2f"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "If the critic is able to evaluate an action a, taken by the actor network in a state s, what should be the actor loss ? (do not search for something to complicated, the answer is simple! :)\n",
        "<div/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d34950c",
      "metadata": {
        "id": "1d34950c"
      },
      "source": [
        "<details class=\"alert alert-danger\">\n",
        "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
        "Note that if we are learning a sample s, a, r, s', d from the buffer, only s can be used in the actor training. In fact, the action chosen by the actor at time step t, will not obviously be the same than the action taken by actor at time step t - n, when the sample was generated in the environment (because the actor is learning at every time step). For this reason, a, r, s', d and no more re-usable for the actor.\n",
        "\n",
        "    a = actor(s)\n",
        "    loss_actor = -critic(s, a)\n",
        "\n",
        "Note that the actor nedd the critic to be trained befor he can learn. In some way, the actor ability is trained to make shure the agent is going in states that satisfy the critic. Generally, in the learn() function, we train the critic before the actor, but is just speed-up the learning by 1 time step so it's not mandatory.\n",
        "<details/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d7e751",
      "metadata": {
        "id": "97d7e751"
      },
      "source": [
        "#### Performances\n",
        "\n",
        "You shouldn't be worry if your DDPG training is long. In my experiments, the learning took 100 episodes to get a sum of reawards around -700 in average, and 200 episodes to converge between -200 and -400 with the following attributes:\n",
        "\n",
        " - Env: Pendulum-v0\n",
        " - Noise: mean=0, std=0.1\n",
        " - actor_learning_rate=0.000025\n",
        " - critic_learning_rate=0.00025, \n",
        " - gamma=0.99,  # Discount factor\n",
        " - buffer_max_size=1000000, \n",
        " - layer1_size=200, # For both critic and actor\n",
        " - layer2_size=150, # For both critic and actor\n",
        " - batch_size=64,  # For both critic and actor\n",
        " - tau=0.01,\n",
        " \n",
        "The last parameter tau is to update target networks. At each time steps, I update the weights of target networks to be a weighted average between their weights, and the targeted network weights :\n",
        "\n",
        "    target_model_weight = target_model_weight * (1 - tau) + model_weight * tau\n",
        "    \n",
        "This is another way to update target critic and target actor, but the one you learned in RL5 notebook (copy weights every n time steps) should also work.\n",
        "\n",
        "Here I give you an architecture for both actor and critic neural networks. The Sequential function we used before should work, but the learning in continuous action space is really long, and use LayerNorm layers increase so much learning speed, so you will gain time on your notebook. Initialise weights as I do here increase a bit more the learning speed, but according to my experience the impact is not really increadible.\n",
        "\n",
        "[WARN] I also recommand to use torch.tanh activation function on the last layer of the actor since it increase the learning.\n",
        "\n",
        "I can't explain why all of these increase the learning speed, I just observed it guided by some tips found on internet ... Everithing you should understand and remember here, is why the default architecture and behaviour of DDPG make the learning converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce34aab0",
      "metadata": {
        "id": "ce34aab0"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "def init_weights(layer, bound=None):\n",
        "    if bound is None:\n",
        "        bound = 1. / np.sqrt(layer.weight.data.size()[0])\n",
        "    torch.nn.init.uniform_(layer.weight.data, -bound, bound)\n",
        "    torch.nn.init.uniform_(layer.bias.data, -bound, bound)\n",
        "\n",
        "\n",
        "class DefaultNN(nn.Module):\n",
        "    def __init__(self, learning_rate, input_dims, layer_1_dims, layer_2_dims, output_dims, device,\n",
        "                 last_activation=None):\n",
        "        super().__init__()\n",
        "        self.last_activation = last_activation\n",
        "        self.layer_1 = nn.Linear(input_dims, layer_1_dims)\n",
        "        init_weights(self.layer_1)\n",
        "        self.layer_norm_1 = nn.LayerNorm(layer_1_dims)\n",
        "\n",
        "        self.layer_2 = nn.Linear(layer_1_dims, layer_2_dims)\n",
        "        init_weights(self.layer_2)\n",
        "        self.layer_norm_2 = nn.LayerNorm(layer_2_dims)\n",
        "\n",
        "        self.layer_3 = nn.Linear(layer_2_dims, output_dims)\n",
        "        init_weights(self.layer_3, bound=0.003)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        running_output = self.layer_1(inputs)\n",
        "        running_output = self.layer_norm_1(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_2(running_output)\n",
        "        running_output = self.layer_norm_2(running_output)\n",
        "        running_output = torch.nn.functional.relu(running_output)\n",
        "        running_output = self.layer_3(running_output)\n",
        "\n",
        "        if self.last_activation is not None:\n",
        "            running_output = self.last_activation(running_output)\n",
        "        return running_output\n",
        "\n",
        "    def converge_to(self, other_model, tau=0.01):\n",
        "        \"\"\"\n",
        "        Make the value of parameters of this model converge to one from the given model.\n",
        "        The parameter tau indicate how close our weights should be from the one of the other model.\n",
        "        self.converge_to(other_model, tau=1) is equivalent to self = copy.deepcopy(other_model).\n",
        "\n",
        "        other_model should have the same shape, dimensions, than self.\n",
        "        \"\"\"\n",
        "        for self_param, other_param in zip(self.parameters(), other_model.parameters()):\n",
        "            self_param.data.copy_(\n",
        "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import odeint\n",
        "\n",
        "class CarPend:\n",
        "    def __init__(self,max_time=400):\n",
        "        self.M = 400/1000\n",
        "        self.m = 50/1000\n",
        "        self.action_space = gym.spaces.Box(-5.0,5.0,(1,),np.float64)\n",
        "        self.observation_space=gym.spaces.Box(np.array([-5.0,-10.0,-2.0*np.pi,-10.0]),np.array([5.0,10.0,2.0*np.pi,10.0]),(4,),np.float64)\n",
        "        self.dt = 0.01\n",
        "        self.count=0\n",
        "        self.L = 0.3\n",
        "        \n",
        "        self.x = 0\n",
        "        self.xp = 0\n",
        "        self.xpp = 0\n",
        "        \n",
        "        self.theta = 0\n",
        "        self.thetap = 0\n",
        "        self.thetapp = 0\n",
        "        self.max_time=max_time\n",
        "        \n",
        "    def reset(self):\n",
        "        self.x = 0\n",
        "        self.xp = 0\n",
        "        self.count=0\n",
        "        self.theta = np.random.uniform(-np.pi/100., np.pi/100.)\n",
        "        self.thetap = 0#np.random.uniform(-0.1,0.1)\n",
        "\n",
        "        state = [self.x,self.xp,self.theta,self.thetap]\n",
        "        return np.array(state)\n",
        "\n",
        "    def dynamic_mod(self,x0,t,f):\n",
        "        x1,x2,x3,x4 = x0\n",
        "        tol = 10**(-6)\n",
        "        er = 1\n",
        "        g = 9.81\n",
        "        b = 0.01\n",
        "        \n",
        "        dx2dt_old = self.xpp\n",
        "        dx4dt_old = self.thetapp\n",
        "        \n",
        "        while er>tol:\n",
        "            if abs(x1)>=5:\n",
        "                dx2dt = 0\n",
        "            else:\n",
        "                dx2dt = (f - dx4dt_old * self.m * self.L * np.cos(x3) + self.m * self.L * (x4**2) * np.sin(x3)) / (self.M + self.m)\n",
        "            \n",
        "            dx4dt = (self.m * g * self.L * np.sin(x3) - self.m * self.L * np.cos(x3) * dx2dt - b*x4) / (self.m * self.L**2)\n",
        "            \n",
        "            er = max(abs(dx2dt - dx2dt_old),abs(dx4dt - dx4dt_old))\n",
        "            \n",
        "            dx2dt_old = dx2dt\n",
        "            dx4dt_old = dx4dt\n",
        "        \n",
        "        self.xpp = dx2dt\n",
        "        self.thetapp = dx4dt\n",
        "        return np.array([float(x2),float(dx2dt),float(x4),float(dx4dt)])\n",
        "    \n",
        "    def step(self,action):\n",
        "        done=False\n",
        "        self.count+=1\n",
        "        x0 = np.array([self.x,self.xp,self.theta,self.thetap])\n",
        "        t=np.array([0,self.dt])\n",
        "        self.action=action\n",
        "        \n",
        "        #print(self.action.dtype)\n",
        "        sol = odeint(self.dynamic_mod,x0,t,args=(self.action,))\n",
        "        \n",
        "        self.x = sol[-1,0]\n",
        "        self.x = max(self.x,-5)\n",
        "        self.x = min(self.x, 5)\n",
        "        \n",
        "        if abs(self.x)>=5:\n",
        "            self.xp = 0\n",
        "        else:\n",
        "            self.xp = sol[-1,1]\n",
        "        \n",
        "        self.theta = sol[-1,2]\n",
        "        self.thetap = sol[-1,3]\n",
        "        \n",
        "        state = [self.x,self.xp,self.theta,self.thetap]\n",
        "        \n",
        "        if abs(self.x)<=2.5 and abs(self.theta)<=50*np.pi/180:\n",
        "            done = False\n",
        "            #reward = 1\n",
        "            \n",
        "        else:\n",
        "            done = True\n",
        "            #reward = 0\n",
        "\n",
        "        reward=-100*self.theta**2 \n",
        "        \n",
        "        if self.count>=self.max_time:\n",
        "          done=True\n",
        "            \n",
        "        return np.array(state), reward, done,_"
      ],
      "metadata": {
        "id": "DL7WU13AtCIJ"
      },
      "id": "DL7WU13AtCIJ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulation(environment, agent, nb_episodes=200, verbose=True):\n",
        "    episodes_rewards_sum = []\n",
        "    agent.on_simulation_start()\n",
        "    for episode_id in range(nb_episodes):\n",
        "        state = environment.reset()\n",
        "        agent.on_episode_start(state)\n",
        "\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.action(state)\n",
        "            state, reward, done ,_= environment.step(action)\n",
        "            # Ending time step process ...\n",
        "            agent.on_action_stop(action, state, reward, done)\n",
        "\n",
        "            # Store reward\n",
        "            episode_rewards.append(reward)\n",
        "        agent.on_episode_stop()\n",
        "        rewards_sum = sum(episode_rewards)\n",
        "        episodes_rewards_sum.append(rewards_sum)\n",
        "        #environment.close()\n",
        "        if len(episodes_rewards_sum) > 20:\n",
        "            last_20_average = mean(episodes_rewards_sum[-20:])\n",
        "        else:\n",
        "            last_20_average = mean(episodes_rewards_sum)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Episode \", '{:3d}'.format(episode_id),\n",
        "                  \", episode return \", '{:4.1f}'.format(rewards_sum),\n",
        "                  \", last 20 avg \", '{:4.1f}'.format(last_20_average),\n",
        "                  sep='')\n",
        "    return episodes_rewards_sum"
      ],
      "metadata": {
        "id": "d6RjX_UNQj_y"
      },
      "id": "d6RjX_UNQj_y",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9e89db5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "9e89db5f",
        "outputId": "e74f79bb-6463-4e7b-8b78-c09a43814b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode   0, episode return -871.5, last 20 avg -871.5\n",
            "Episode   1, episode return -946.6, last 20 avg -909.1\n",
            "Episode   2, episode return -779.6, last 20 avg -865.9\n",
            "Episode   3, episode return -722.9, last 20 avg -830.1\n",
            "Episode   4, episode return -676.8, last 20 avg -799.5\n",
            "Episode   5, episode return -633.0, last 20 avg -771.7\n",
            "Episode   6, episode return -631.8, last 20 avg -751.7\n",
            "Episode   7, episode return -666.1, last 20 avg -741.0\n",
            "Episode   8, episode return -594.3, last 20 avg -724.7\n",
            "Episode   9, episode return -580.0, last 20 avg -710.3\n",
            "Episode  10, episode return -610.4, last 20 avg -701.2\n",
            "Episode  11, episode return -574.6, last 20 avg -690.6\n",
            "Episode  12, episode return -585.4, last 20 avg -682.5\n",
            "Episode  13, episode return -572.3, last 20 avg -674.7\n",
            "Episode  14, episode return -551.2, last 20 avg -666.4\n",
            "Episode  15, episode return -535.1, last 20 avg -658.2\n",
            "Episode  16, episode return -580.8, last 20 avg -653.7\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5f8704b773eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCarPend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-74a0ac84f496>\u001b[0m in \u001b[0;36msimulation\u001b[0;34m(environment, agent, nb_episodes, verbose)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Ending time step process ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Store reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-5f8704b773eb>\u001b[0m in \u001b[0;36mon_action_stop\u001b[0;34m(self, action, new_state, reward, done)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-5f8704b773eb>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# %load solutions/RL6_DDPG.py\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
        "\n",
        "### NEW indicate the differences between DQN and DDPG.\n",
        "class DDPGAgent(Agent):\n",
        "    def __init__(self, state_space, action_space, device, actor_lr=0.000025, critic_lr=0.00025, tau=0.001, gamma=0.99,\n",
        "                 max_size=100000, layer1_size=200, layer2_size=150, batch_size=64, noise_std=0.1, name=\"DDPG\"):\n",
        "        assert isinstance(action_space, gym.spaces.Box)  ### NEW: The action space is now continuous \n",
        "        super().__init__(state_space, action_space, device=device, name=name)\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.replay_buffer = ReplayBuffer(max_size, self.device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.actor = DefaultNN(actor_lr, self.state_size, layer1_size, layer2_size, self.nb_actions\n",
        "                               , self.device,\n",
        "                               last_activation=None)\n",
        "        self.critic = DefaultNN(critic_lr, self.state_size + self.nb_actions, layer1_size, layer2_size, 1, self.device)\n",
        "\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.normal_distribution = torch.distributions.normal.Normal(\n",
        "            torch.zeros(self.nb_actions), torch.full((self.nb_actions,), noise_std))\n",
        "\n",
        "    def action(self, observation):\n",
        "        with torch.no_grad():\n",
        "            observation = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
        "            actor_output = self.actor.forward(observation).to(self.device)\n",
        "            #action=torch.normal(actor_output[0], np.abs(actor_output[1]))\n",
        "            noise = self.normal_distribution.sample().to(self.device)\n",
        "            action = actor_output + noise\n",
        "        return action.cpu().detach().numpy()\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
        "        self.learn()\n",
        "        super().on_action_stop(action, new_state, reward, done)\n",
        "\n",
        "    def learn(self):\n",
        "        #if len(self.replay_buffer) > self.batch_size:\n",
        "        if len(self.replay_buffer.data) > self.batch_size:\n",
        "            states, actions, rewards, new_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                target_actions = self.target_actor.forward(new_states)\n",
        "                critic_value_ = self.target_critic.forward(torch.concat((new_states, target_actions), dim=-1))\n",
        "            critic_value = self.critic.forward(torch.concat((states, actions), dim=-1))\n",
        "            target = torch.addcmul(rewards, self.gamma, 1 - dones, critic_value_.squeeze()).view(self.batch_size, 1)\n",
        "            self.critic.optimizer.zero_grad()\n",
        "            critic_loss = torch.nn.functional.mse_loss(target, critic_value)\n",
        "            critic_loss.backward()\n",
        "            self.critic.optimizer.step()\n",
        "\n",
        "            self.actor.optimizer.zero_grad()\n",
        "            actions = self.actor.forward(states)\n",
        "            actor_loss = - self.critic.forward(torch.concat((states, actions), dim=-1))\n",
        "            actor_loss = torch.mean(actor_loss)\n",
        "            actor_loss.backward()\n",
        "            self.actor.optimizer.step()\n",
        "\n",
        "            self.target_critic.converge_to(self.critic, tau=self.tau)\n",
        "            self.target_actor.converge_to(self.actor, tau=self.tau)\n",
        "\n",
        "# Test our agent on Pendulum-v0\n",
        "#environment = gym.make('Pendulum-v1')\n",
        "environment = CarPend()\n",
        "agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
        "simulation(environment, agent,nb_episodes=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(agent.actor.state_dict(), 'act_checkpoint.pth')\n",
        "#agent.critic.save('act_checkpoint.pth')\n",
        "torch.save(agent.critic.state_dict(), 'crit_checkpoint.pth')"
      ],
      "metadata": {
        "id": "f_Lf95wigovM"
      },
      "id": "f_Lf95wigovM",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There. we finished\n",
        "# Lets see it in action\n",
        "cartpole=CarPend()\n",
        "done = False\n",
        "cnt = 0\n",
        "observation = cartpole.reset()\n",
        "t=[]\n",
        "x=[]\n",
        "xp=[]\n",
        "th=[]\n",
        "thp=[]\n",
        "while not done :\n",
        "    cnt += 1\n",
        "    t.append(cnt)\n",
        "\n",
        "    action = agent.action(observation)\n",
        "    observation, reward, done,_ = cartpole.step(action)\n",
        "    x.append(observation[0])\n",
        "    xp.append(observation[1])\n",
        "    th.append(observation[2])\n",
        "    thp.append(observation[3])\n",
        "\n",
        "\n",
        "\n",
        "    # Lets see how long it lasts until failing\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('t (step)')\n",
        "plt.ylabel('x')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t,xp)\n",
        "plt.xlabel('t (step)')\n",
        "plt.ylabel('v')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t,np.array(th)*180/np.pi)\n",
        "plt.xlabel('t (step)')\n",
        "plt.ylabel('theta')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(t,thp)\n",
        "plt.xlabel('t (step)')\n",
        "plt.ylabel('theta point')\n",
        "#plt.figure()\n",
        "#plt.plot(t,xpp)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gSC_kDdjiWTh",
        "outputId": "90af56a3-0b3c-4979-d52d-f82a227b5e09"
      },
      "id": "gSC_kDdjiWTh",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhV5bn+8e9DZkLClDDPgkyKiBGc60ipEx2ch2Kr1Z9H22M9erStVQ+ettra2p7WWmml1lmrHajVKo44oYRBUMYwhykJkBDInP38/tgLGzFggKysJPv+XNe+9hqzn6Wb3FnrXet9zd0RERHZU4eoCxARkdZJASEiIo1SQIiISKMUECIi0igFhIiINCo56gKaS05Ojg8aNCjqMkRE2pS5c+eWuHtuY+vaTUAMGjSI/Pz8qMsQEWlTzGzt3tbpEpOIiDRKASEiIo1SQIiISKMUECIi0igFhIiINCrUgDCzSWa2zMwKzOzWRtbfaGaLzWyhmb1qZgMbrKs3swXBa0aYdYqIyGeFdpurmSUB9wNnAIXAHDOb4e6LG2w2H8hz9wozuxb4KXBhsK7S3ceGVZ+IiOxbmGcQ44ECd1/l7jXAU8Dkhhu4++vuXhHMzgb6hViPiEi787f5G/jLvELCGLohzIDoC6xvMF8YLNubK4EXG8ynm1m+mc02sy83toOZXR1sk19cXHzwFYuItCGVNfX87z+X8OzcQsys2X9+q3iS2swuA/KALzRYPNDdN5jZEOA1M1vk7isb7ufu04BpAHl5eRr5SEQSymOz11Kys5oHLhsXys8P8wxiA9C/wXy/YNmnmNnpwA+Ac929evdyd98QvK8C3gCODLFWEZE2ZVd1Hb97cyUnDsvh6EHdQvmMMANiDjDMzAabWSpwEfCpu5HM7EjgQeLhUNRgeVczSwumc4DjgYaN2yIiCe2R99aydVcN3z3j0NA+I7RLTO5eZ2bXAy8BScB0d//YzKYC+e4+A/gZ0An4c3D9bJ27nwuMBB40sxjxELt7j7ufREQSVnlVLQ/OWskpw3MZN6BraJ8TahuEu78AvLDHstsbTJ++l/3eBQ4PszYRkbbq4XfWUFpRG+rZA+hJahGRNqWsspbfv7WK00f2ZEy/LqF+lgJCRKQNmf72anZU1XHD6cNC/ywFhIhIG1FaUcP0t1czaXQvDuvbOfTPU0CIiLQRf3hrNeXVddxwRvhnD6CAEBFpE7btquGP76zmrDG9GdEru0U+UwEhItIGTJu1ioraem44rWXOHkABISLS6hWVV/Gnd9dwzpg+DOuZ1WKfq4AQEWnlfvv6SmrqY6E/97AnBYSISCu2flsFj7+/lgvy+jE4J7NFP1sBISLSiv3q1RWYGd9pwbaH3RQQIiKtVEFROX+ZV8jXjxlI784ZLf75CggRkVbq5y8vJyMliWtPPiSSz1dAiIi0QgsLS3nxo81cdeIQundKi6QGBYSISCt078vL6doxhatOHBxZDQoIEZFWZvaqrcxaXsx/nDyUrPSUyOpQQIiItCLuzs9eWkbP7DQuP3ZgpLUoIEREWpHXlxUxd+12vnPaMNJTkiKtRQEhItJKxGLOz15azsDuHbkgr3/U5SggRERaixkfbmTJph3ceMahpCRF/+s5+gpERITqunrufXkZo/tkc86YPlGXAyggRERahUffW0vh9kq+96WRdOhgUZcDKCBERCJXVlnLb14v4MRhOZwwLCfqcj6hgBARidgDb6ykrLKWW780IupSPkUBISISoY2llfzxndV8ZWxfRvfpHHU5n6KAEBGJ0H0zl+MON05s2cGAmkIBISISkaWbd/DsvEKmHDeQfl07Rl3OZyggREQics+LS8lKS+a6U4ZGXUqjFBAiIhF4d2UJry8r5rpThtKlY2rU5TRKASEi0sJiMefuF5fSp3M6U44bFHU5e6WAEBFpYc8v2sTCwjJunDg88g759iXUgDCzSWa2zMwKzOzWRtbfaGaLzWyhmb1qZgMbrJtiZiuC15Qw6xQRaSlVtfXc8+JSRvXO5itH9o26nH0KLSDMLAm4H/gSMAq42MxG7bHZfCDP3ccAzwI/DfbtBtwBTADGA3eYWdewahURaSkPvb2aDaWV3Hb2SJJaSZcaexPmGcR4oMDdV7l7DfAUMLnhBu7+urtXBLOzgX7B9BeBme6+zd23AzOBSSHWKiISuqLyKn77egFnjOrJcYe0ni419ibMgOgLrG8wXxgs25srgRcPcF8RkVbvFy8vp6Y+xvfPHBl1KU2SHHUBAGZ2GZAHfGE/97sauBpgwIABIVQmItI8Fm/cwdP56/nm8YMZnJMZdTlNEuYZxAag4ZBI/YJln2JmpwM/AM519+r92dfdp7l7nrvn5ebmNlvhIiLNyd35338upnNGCt85dVjU5TRZmAExBxhmZoPNLBW4CJjRcAMzOxJ4kHg4FDVY9RIw0cy6Bo3TE4NlIiJtzqtLinh35VZuOG0YnTumRF1Ok4V2icnd68zseuK/2JOA6e7+sZlNBfLdfQbwM6AT8GczA1jn7ue6+zYzu4t4yABMdfdtYdUqIhKWmroYP35hCUNyM7n0mIGfv0MrEmobhLu/ALywx7LbG0yfvo99pwPTw6tORCR8j81ey6qSXTw0Ja9VjDO9P9pWtSIibUhpRQ2/enUFJwzN4dQRPaIuZ78pIEREQvLLV1ZQXlXLbWePJLiM3qYoIEREQrBsczmPzl7LxeMHMKJXdtTlHBAFhIhIM3N37pzxMZ3Skrlp4vCoyzlgCggRkWb2wqLNvLdqKzdNPJSuma1zrIemUECIiDSjypp6fvTPxYzsnc0lE9rWba17UkCIiDSjB94oYGNZFXeeM6rV99b6eRQQIiLNZP22Cn43axXnHtGHCUO6R13OQVNAiIg0k7ueX0ySGd87c0TUpTQLBYSISDOYtbyYlxdv4fpTh9K7c0bU5TQLBYSIyEGqqYvxP//4mIHdO3LViYOjLqfZKCBERA7SI++tYWXxLm4/exRpyUlRl9NsFBAiIgdhc1kV981czinDczltZM+oy2lWCggRkYNw1z8XUxdz7jx3dNSlNDsFhIjIAZq1vJh/LtzEdacMZWD3tjGM6P5QQIiIHICq2npu//tHDM7J5JovDIm6nFCEOmCQiEh79cAbK1mztYLHrpzQrhqmG9IZhIjIflpdsosH3lzJOUf04YRhOVGXExoFhIjIfnB3bv/7R6QldeCHZ42MupxQKSBERPbDPxdt4q0VJfzXxEPpkZ0edTmhUkCIiDRReVUtU/+xmNF9srnsmLbdlXdTqJFaRKSJ7pu5guKd1Uz7eh7JSe3/7+v2f4QiIs1gUWEZD7+7movHD2Bs/y5Rl9MiFBAiIp+jtj7GLc8tpHunNG6Z1D668m4KXWISEfkcD729msWbdvDApePonJESdTktRmcQIiL7sKZkF/fNXM4Zo3oy6bBeUZfTohQQIiJ74e784G+LSE3qwF2TD8OsbY8xvb8UECIie/Hs3ELeKdjKf39pBL06t+9nHhqjgBARaURxeTX/+88l5A3syqXjB0RdTiQUECIijZj6/GIqaur4yVcPp0OHxLq0tJsCQkRkD68vLeIfH27kulOGMqxnVtTlRCbUgDCzSWa2zMwKzOzWRtafZGbzzKzOzM7bY129mS0IXjPCrFNEZLed1XXc9rePGNqjE9eefEjU5UQqtOcgzCwJuB84AygE5pjZDHdf3GCzdcAVwE2N/IhKdx8bVn0iIo35yQtL2FhWyZ+vObbdjvPQVGE+KDceKHD3VQBm9hQwGfgkINx9TbAuFmIdIiJN8k5BCY+/v44rTxhM3qBuUZcTuTAvMfUF1jeYLwyWNVW6meWb2Wwz+3JjG5jZ1cE2+cXFxQdTq4gkuJ3Vdfz3swsZnJPJTROHR11Oq9CaG6kHunsecAnwSzP7zMVAd5/m7nnunpebm9vyFYpIu/Hj4NLSveePISM1sS8t7RZmQGwA+jeY7xcsaxJ33xC8rwLeAI5szuJERHZ7e0UJT7y/jqtOGMxRA3VpabcwA2IOMMzMBptZKnAR0KS7kcysq5mlBdM5wPE0aLsQEWku5VW13PLcQobkZvJfurT0KaEFhLvXAdcDLwFLgGfc/WMzm2pm5wKY2dFmVgicDzxoZh8Hu48E8s3sQ+B14O497n4SEWkWP35hKZvKKvnZeUeQnqJLSw2F2t23u78AvLDHstsbTM8hfulpz/3eBQ4PszYRkbdWFPPkB+u45qQhHDWwa9TltDqtuZFaRCQ05VW13PLsQg7JzeS7ZxwadTmtkgYMEpGENPUfi9m8o4rnrj1Ol5b2QmcQIpJwXly0iT/PLeS6U4Zy5ABdWtobBYSIJJTNZVV876+LOKJfZ75z2rCoy2nVFBAikjBiMefmZz+kujbGfReOJSVJvwL3Rf91RCRhPPzuGt5aUcIPzx7FkNxOUZfT6ikgRCQhLNtczt3/WsrpI3tw8fj+n7+DKCBEpP2rrqvnP5+aT3Z6Mnd/bQxmiTlC3P7Sba4i0u7d+9Iylm4uZ/oVeeR0Sou6nDZDZxAi0q69W1DC799azWXHDODUET2jLqdNUUCISLtVsrOaG55ewJDcTH5w5qioy2lzFBAi0i7FYs6Nz3xIaWUtv7l4nMZ4OAAKCBFplx6ctYpZy4u545xRjOqTHXU5bdLnBoSZfea8zMxODqUaEZFmMHftNu59eRlnjenNJeMHRF1Om9WUM4hnzOwWi8sws18DPwm7MBGRA1FaUcO3n5hP3y4Z/OSrh+uW1oPQlICYQHzo0HeJjxK3kfgIbyIirYq7c9OfF1K8s5rfXHIk2ekpUZfUpjUlIGqBSiADSAdWu3ss1KpERA7A9HfW8MqSLXz/zJGM6dcl6nLavKYExBziAXE0cCJwsZn9OdSqRET204frS7n7xSWcMaonVxw3KOpy2oWmPEl9pbvnB9ObgMlmdnmINYmI7JfSihque2IePbLS+dl56kqjuXxuQDQIh4bLHg2nHBGR/VMfc77z1AKKdlTz9DXH0KVjatQltRvqi0lE2rRfvrKcWcuL+fFXDtfocM1MD8qJSJs1c/EWfv1aARfk9VMX3iFQQIhIm7S6ZBc3Pr2Aw/t2Zurkw9TuEAIFhIi0Obuq67jm0XySk4wHLhtHeor6WQqD2iBEpE1xd255biEFRTv50zfH069rx6hLard0BiEibcpDb6/m+YWbuOmLwzlxWG7U5bRrCggRaTPeWlHMT15cyhdH9+TaLxwSdTntngJCRNqEgqKd/Mfj8xia24l7zz9CjdItQAEhIq1eaUUNV/1pDqlJHfjDlDyy1Alfi1AjtYi0arX1Ma59bB4bS6t44lsT6N9NjdItRQEhIq2Wu3P73z/mvVVb+fn5R5A3qFvUJSWUUC8xmdkkM1tmZgVmdmsj608ys3lmVmdm5+2xboqZrQheU8KsU0Rap4ffXcOTH6zj2pMP4WtH9Yu6nIQTWkCYWRJwP/AlYBTxbsL3HL50HXAF8MQe+3YD7iA+WNF44A4zUycrIgnkjWVF3PX8YiaO6snNE4dHXU5CCvMMYjxQ4O6r3L0GeAqY3HADd1/j7guBPQcg+iIw0923uft2YCYwKcRaRaQVWba5nG8/MZ/hvbK578KxdOigO5aiEGZA9AXWN5gvDJY1275mdrWZ5ZtZfnFx8QEXKiKtx8bSSqZM/4CM1CT+MCWPzDQ1lUalTd/m6u7T3D3P3fNyc/VEpUhbV1ZRyxV//IBd1XU8/I3x9O2SEXVJCS3MgNgANOx/t1+wLOx9RaQNqqqt51uP5rO6ZBcPXn4Uo/pkR11SwgszIOYAw8xssJmlAhcBM5q470vARDPrGjROTwyWiUg7VB9zbnxmAR+s3sbPLxjLcUNzoi5JCDEg3L0OuJ74L/YlwDPu/rGZTTWzcwHM7GgzKwTOBx40s4+DfbcBdxEPmTnA1GCZiLQz7s5dzy/mhUWbue2skZx7RJ+oS5KAuXvUNTSLvLw8z8//zPDZItLK/e7Nldz94lKuPGEwPzx7zzvhJWxmNtfd8xpb16YbqUWkbXtubiF3v7iUs8f05gdnjoy6HNmDAkJEIvHCok3c/OyHHD+0Oz+/4Ag969AKKSBEpMW9tnQL33lyPuMGdOX3X88jLVlDhrZGCggRaVHvFpTw/x6bx4jeWUz/xtF0TNWDcK2VAkJEWszctdu46pF8BnXvyCPfnEC2xnVo1RQQItIiPtpQxhV/nEOPrDQeu3IC3TJToy5JPocCQkRCt2JLOZc/9D7Z6Sk8/q1j6JGdHnVJ0gQKCBEJ1dLNO7j497NJTurA41dNUP9KbYgCQkRC89GGMi6eNpukDsZTVx/DoJzMqEuS/aCAEJFQLFhfyiW/n03H1GSeueZYDsntFHVJsp90f5mINLv8Ndu44o9z6JaZyhPfmkC/rh2jLkkOgM4gRKRZvbdyK1+f/gE9stJ4+ppjFA5tmM4gRKTZvLWimG89kk//rh15/KoJulupjVNAiEiz+MeHG/mvZz5kSG4mj181ge6d0qIuSQ6SLjGJyEGb/vZqvv3kfMb278LTVx+rcGgndAYhIgcsFnPu+ddSHpy1ikmje/HLi8aSnqKO99oLBYSIHJCauhi3PLeQv87fwOXHDOTOc0eTpC672xUFhIjst53VdVz72FzeWlHCTRMP5bpThmKmcGhvFBAisl82l1Vx1SNzWLKpnJ+eN4YL8vpHXZKERAEhIk22YH0pVz+Sz67qOv7w9TxOGdEj6pIkRAoIEWmSvy/YwM3PLqRHVhqPXHkcI3plR12ShEwBISL7FIs59768jN++sZLxg7vxwKXjdBtrglBAiMhe7ayu47tPL2Dm4i1cdHR/pk4+jNRkPT6VKBQQItKotVt3cc2jc1m+pZw7zhnFFccN0p1KCUYBISKf8a+PNnPzsx9iwMPfGM9Jh+ZGXZJEQAEhIp+orY9xz4tL+cPbqxnTrzP3XzKO/t3UG2uiUkCICAAbSyu5/ol5zFtXypRjB/L9s0aSlqxuMxKZAkJEeHN5MTc8NZ+auhi/vvhIzjmiT9QlSSuggBBJYDV1Me57ZTm/e3Mlw3tmcf+l4zQ0qHxCASGSoJZvKeeGpxaweNMOLjq6P3ecM5qMVF1Skn8L9YZmM5tkZsvMrMDMbm1kfZqZPR2sf9/MBgXLB5lZpZktCF6/C7NOkUQSiznT317N2b9+m807qph2+VHc/bUxCgf5jNDOIMwsCbgfOAMoBOaY2Qx3X9xgsyuB7e4+1MwuAu4BLgzWrXT3sWHVJ5KINpdVcfOzH/LWihJOHdGDe742htwsPRUtjQvzEtN4oMDdVwGY2VPAZKBhQEwG7gymnwV+Y3oSR6TZuTszPtzI7X//mJq6GD/+yuFcPL6/HnyTfQozIPoC6xvMFwIT9raNu9eZWRnQPVg32MzmAzuA29z9rT0/wMyuBq4GGDBgQPNWL9JObCqr5La/fsSrS4sY278L9104lsE5mVGXJW1Aa22k3gQMcPetZnYU8DczG+3uOxpu5O7TgGkAeXl5HkGdIq1WLOY8/v5a7vnXMupiMW47ayTfOH6wRn2TJgszIDYADUcS6Rcsa2ybQjNLBjoDW93dgWoAd59rZiuBQ4H8EOsVaTcKinbyvb8sZM6a7Zw4LIcffflwBnTXE9Gyf8IMiDnAMDMbTDwILgIu2WObGcAU4D3gPOA1d3czywW2uXu9mQ0BhgGrQqxVpF2oqq1n2qxV/Oa1AjJSk7j3/CP42ri+amuQAxJaQARtCtcDLwFJwHR3/9jMpgL57j4DeAh41MwKgG3EQwTgJGCqmdUCMeD/ufu2sGoVaevcnVeWFHHX84tZt62Cs8b05s5zRusOJTkoFr+a0/bl5eV5fr6uQEniKSjaydTnFzNreTHDenTijnNGc8KwnKjLkjbCzOa6e15j61prI7WIfI4dVbX83ysrePjdNWSkJnH72aO4/NiBpCRpQB9pHgoIkTamtj7GUx+s41evrmDrrhouzOvPTV8cTo6GAZVmpoAQaSNiMecfCzfyi5nLWbu1gvGDujH9ipGM6dcl6tKknVJAiLRy7s6sFSX89F9L+XjjDkb0yuKPVxzNycNzdXeShEoBIdKKzVmzjV+8vJz3Vm2lf7cMfnnhWM49og8d9LCbtAAFhEgr4+68t3Ir//faCmav2kZOp1TuPGcUl0wYSGqyGqCl5SggRFoJd+fN5cX8+rUC5q7dTo+sNH549iguGT9AXXFLJBQQIhGrjzkzF2/mt2+sZGFhGX06p3PX5NGcn9ef9BQFg0RHASESkZ3Vdfw5fz3T31nN+m2VDOjWkbu/ejhfHddPl5KkVVBAiLSwDaWV/OndNTz5wTrKq+rIG9iVH5w5kjNG9VJPq9KqKCBEWsDuhufHP1jHvz7aDMCXDuvFlScM5sgBXSOuTqRxCgiREG3bVcNzcwt58oN1rCrZReeMFL55/CCmHDeIfl3V/ba0bgoIkWbm7nywehtPfrCOFxZtpqY+xlEDu/KLU4dy5uG91fAsbYYCQqSZrN26i7/M28Bf529g3bYKstKSuXh8fy6ZMJDhvbKiLk9kvykgRA5CWWUtLyzaxHNzC8lfux0zOP6QHG44fRiTDutFx1T9E5O2S99ekf20s7qOV5ds4fmFm3hzeTE1dTGG9ujELZNG8OUj+9C7c0bUJYo0CwWESBM0Fgq9stO5dMIAvnJkXw7v21kd50m7o4AQ2YuiHVW8urSIVxZv4a2CEmrqYvTMTuPSCQM46/DejBvQVZ3mSbumgBAJuDtLN5fzyuItvLK0iA/XlwLQr2uGQkESkgJCElpZRS3vrCxh1vJiZi0vZmNZFQBj+3fh5i8O5/SRPTm0ZyddPpKEpICQhFJbH2NhYRlvryjhzeVFLFhfSswhKy2Z44fm8O3TcjltRA96ZKdHXapI5BQQ0q7tDoTZq7Yye9VW5q7dTkVNPWYwpm9nrj9lKCcdmsvY/l1ITlIHeSINKSCkXSmrqGX++u3MW1fKvLXbmbt2O5W19QAM75nF+Uf145gh3ZkwpDvdMlMjrlakdVNASJtVWx9j+ZZyFhWWMW9dPBQKinYC0MFgeK9sLsiLB8L4wd3o3ikt4opF2hYFhLQJtfUxCop2sqiwjEUbyli4oYwlm3ZQUxcDoEvHFMYN6MqXx/Zh3ICujOnfhU5p+nqLHAz9C5JWxd0p3lnN0k3lLN28g6WbylmyuZyConJq6x2ATmnJjO6TzZRjB3JY386M6deFQd076k4jkWamgJBIxGLOxrJKCop2UlC0k5XFOz+Z3l5R+8l2vbLTGdE7iy8cmsvI3lkc3rczg7pn6lkEkRaggJDQ7D4bWFNSwZqSXazeuiv+XrKLNVt3UVUb+2Tbrh1TGNqjE5MO68XQHlmM7J3FiF7ZakgWiZACQg6Yu1NWWcvG0io2lFayflsF67ZVsH5bBeu3x6cbhkByB2NA944M7p7J8UNzGJKbydDcTgzt0UkNyCKtkAJCGlUfc7buqqZoRzVbdlSxZUc1ReVVbC6rYmNZFRtLK9lYWklFTf2n9stMTaJ/t44M7J7JicNy6d81g0E5mQzOyaRvlww9ayDShiggEkhlTT1bd1WzfVdt/L2ihpLyGkp2VlO8s5qSnTWUlFdTsrOarbtqqI/5Z35GTqdU+nTJYGhuJ04alkufLun06ZJBny4ZDOjWka4dU9RYLNJOhBoQZjYJ+BWQBPzB3e/eY30a8AhwFLAVuNDd1wTrvgdcCdQD33H3l8KstS1wd6rrYuysrqO8qo4dlbXsqKr91HRpRS2llbWUVtTEpyvi09sqaj51uaeh1KQO5HRKJScrjV6d0zmsbza5WWn0yk4nNyudntlp9MxOJzcrjRSdAYgkjNACwsySgPuBM4BCYI6ZzXD3xQ02uxLY7u5Dzewi4B7gQjMbBVwEjAb6AK+Y2aHu/unrGRFzd+piTn3Mqa2PUVcff6+ui1FTH6O6Nv5eUxejuq6eqtoYlbX1VDV4VdbEl1XW1FFRUx9M11NRU09FbT27quuoqK5jZ3V8fV0jf9U3lNzB6NIxhS4dU+mSkUKfLumM7J1Nt8wUumWm0S0zha4dU+neKTV4TyM7PVl/9YvIZ4R5BjEeKHD3VQBm9hQwGWgYEJOBO4PpZ4HfWPw31WTgKXevBlabWUHw895r7iJLK2o473fvEXPHHWLu8VcsHgD1Hg+A+lg8DGKxf4fC5/2ybqrU5A50TE0iIyV4BdOdM1Lo2yWdjqnJdEpLJjMtiY6pyWSlJ5OdnhJ/z0j51HRmapJ+2YtIswgzIPoC6xvMFwIT9raNu9eZWRnQPVg+e499++75AWZ2NXA1wIABAw6oyKQOxvCeWZhBBzM6BO9mhhkkmZGUZCR3MDpY/D0peCUndSClQ3x9SocOJCfFl6UlN3wlkRpMp6ckBa8OZHwynUSS7ukXkVaoTTdSu/s0YBpAXl7eAf05n5Wewv2XjmvWukRE2oMwWxw3AP0bzPcLljW6jZklA52JN1Y3ZV8REQlRmAExBxhmZoPNLJV4o/OMPbaZAUwJps8DXnN3D5ZfZGZpZjYYGAZ8EGKtIiKyh9AuMQVtCtcDLxG/zXW6u39sZlOBfHefATwEPBo0Qm8jHiIE2z1DvEG7Driutd3BJCLS3ln8D/a2Ly8vz/Pz86MuQ0SkTTGzue6e19g6PfUkIiKNUkCIiEijFBAiItIoBYSIiDSq3TRSm1kxsPZzNssBSlqgnNYoUY9dx51YdNz7b6C75za2ot0ERFOYWf7eWuvbu0Q9dh13YtFxNy9dYhIRkUYpIEREpFGJFhDToi4gQol67DruxKLjbkYJ1QYhIiJNl2hnECIi0kQKCBERaVTCBISZTTKzZWZWYGa3Rl1PWMxsupkVmdlHDZZ1M7OZZrYieO8aZY1hMLP+Zva6mS02s4/N7D+D5e362M0s3cw+MLMPg+P+n2D5YDN7P/i+Px10ud/umFmSmc03s+eD+UQ57jVmtsjMFphZfrCs2b/rCREQZpYE3A98CRgFXGxmo6KtKjQPA5P2WHYr8Kq7DwNeDebbmzrgv9x9FHAMcF3w/7i9H3s1cKq7HwGMBSaZ2THAPcB97j4U2A5cGfh44XsAAASkSURBVGGNYfpPYEmD+UQ5boBT3H1sg+cfmv27nhABAYwHCtx9lbvXAE8BkyOuKRTuPov42BoNTQb+FEz/CfhyixbVAtx9k7vPC6bLif/S6Es7P3aP2xnMpgQvB04Fng2Wt7vjBjCzfsBZwB+CeSMBjnsfmv27nigB0RdY32C+MFiWKHq6+6ZgejPQM8piwmZmg4AjgfdJgGMPLrMsAIqAmcBKoNTd64JN2uv3/ZfAfwOxYL47iXHcEP8j4GUzm2tmVwfLmv27HtqIctI6ububWbu9t9nMOgHPATe4+474H5Vx7fXYg9EWx5pZF+CvwIiISwqdmZ0NFLn7XDM7Oep6InCCu28wsx7ATDNb2nBlc33XE+UMYgPQv8F8v2BZothiZr0BgveiiOsJhZmlEA+Hx939L8HihDh2AHcvBV4HjgW6mNnuPwDb4/f9eOBcM1tD/JLxqcCvaP/HDYC7bwjei4j/UTCeEL7riRIQc4BhwR0OqcTHvp4RcU0taQYwJZieAvw9wlpCEVx/fghY4u6/aLCqXR+7meUGZw6YWQZwBvH2l9eB84LN2t1xu/v33L2fuw8i/u/5NXe/lHZ+3ABmlmlmWbungYnAR4TwXU+YJ6nN7Ezi1yyTgOnu/qOISwqFmT0JnEy8+98twB3A34BngAHEu0S/wN33bMhu08zsBOAtYBH/vib9feLtEO322M1sDPEGySTif/A94+5TzWwI8b+suwHzgcvcvTq6SsMTXGK6yd3PToTjDo7xr8FsMvCEu//IzLrTzN/1hAkIERHZP4lyiUlERPaTAkJERBqlgBARkUYpIEREpFEKCBERaZQCQmQfzKyLmf3HPtZnmNmbQYeQe9vm+wdZw71mdurB/AyRA6HbXEX2IejX6Xl3P2wv668Dkt39V/v4GTvdvdNB1DAQ+L27TzzQnyFyIHQGIbJvdwOHBP3u/6yR9ZcSPLFqZr3NbFaw7UdmdqKZ3Q1kBMseD7a7LBjDYYGZPbj77MPMdprZfcG4Dq+aWS6Au68FuptZrxY5YpGAAkJk324FVgb97t/ccEXQbcsQd18TLLoEeMndxwJHAAvc/VagMtj/UjMbCVwIHB9sV088ZAAygXx3Hw28Sfwp+N3mEe9/SKTFqDdXkQOXA5Q2mJ8DTA86Dfybuy9oZJ/TgKOAOUFPsxn8u1O1GPB0MP0Y8JcG+xUBfZqvdJHPpzMIkQNXCaTvngkGazqJeA+iD5vZ1xvZx4A/BWcUY919uLvfuZef37CBMD34PJEWo4AQ2bdyIKuxFe6+HUgys3T4pDF5i7v/nvgoZ+OCTWuDswqIDwV5XtCP/+5xhAcG6zrw755ILwHebvBxhxLvsVOkxSggRPbB3bcC7wSNzo01Ur8MnBBMnwx8aGbzibcz7L6zaRqw0Mwed/fFwG3ERwNbSHwEuN7BdruA8Wb2EfHxDabCJ+NcDAXym/v4RPZFt7mKHAQzGwd8190vb4af1ejtsGb2FWCcu//wYD9DZH/oDELkILj7POD1fT0o1wySgZ+H+PNFGqUzCBERaZTOIEREpFEKCBERaZQCQkREGqWAEBGRRikgRESkUf8fvJ0+C1jH5qoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRV5dn38e+VeU6AhDkQhjAqoEYmcWixzkqtWudhVUWrtnZ4a9X26dM+b+3bqnV4tNqittUqtbZapWpVnKuIGhCQmTDPhCFAIAkZrvePbDSaAAFysk9yfp+19jpn733O3tdeHvzl3sN9m7sjIiLSUFzYBYiISPRROIiISCMKBxERaUThICIijSgcRESkkYSwC2gJubm5XlBQEHYZIiJtyowZMza7e15T69pFOBQUFFBcXBx2GSIibYqZrdzXOp1WEhGRRhQOIiLSiMJBREQaUTiIiEgjCgcREWkkasPBzE4zs0VmVmJmt4Zdj4hILInKcDCzeOB3wOnAEOBiMxsSblUiIrEjWp9zGAmUuPsyADN7GpgAzG/JnSzasJOX5qzDzIgzI87AjM/m4+MIXuunhu8T4oyE+Lj61zgjId5IiIsjKSGY4r/4mpIYT3LwGh9nLXkYIiItLlrDoQewusH8GmBUww+Y2URgIkCvXr0OaSclm8p54K0SWntIi4Q4+0JYpCTufY0nNZhPTUogLTGe1KR40oIpNSmBjOR4MpITyUhJICM5gcyUBNKD14ykBOIUPCLSAqI1HA7I3ScBkwCKiooO6X/vZw7rxpnDzsTdcYc6d+ocHKeuDmrdqa1z6uqcWv/8taa2fnlNnVNTV/fZfHVtHXtq6qiqraO6po49e+dr6qiqrqWqpo7K6jqqamqprK6jsqaWyupaqqrrqKyupaK6lt17atiyq35+954adlfVsru6ltq6Ax+iGWQmJ5CVmkhWSiJZqQlkpyaSk5pETloi2Wmfv89JTaRDehId05PokJZEUkJUnmEUkZBEazisBfIbzPcMlkWEmWEGcUTnX93uzp7aOir21LJrTy3llTWUV1VTXvX5+52VNeyoqGbHZ6/V7KioYfnmXWyvKGPb7mr21NTtcx+ZyQl0SE+iQ3oSuelJ5GYkk5uZRKf0ZHIzk8nNSKJzZgpdspLJTElsxaMXkTBEazh8DBSaWR/qQ+Ei4JJwSwqPmZGcEE9yQjw5aYe+ncrqWsp2V1NWsYdtu6op272HLbv2sG3XHrbu3sPWXfXT+u2VfLp2O1t27WmyxZKeFE+XrBQ6ZyXTNSuFLtkp9MhJpXt2Kt1zUumRk0pWagJm0Rm2InJgURkO7l5jZjcBrwLxwB/dfV7IZbV5KYnxdM2Op2t2SrM+X1fnlFVUs6W8itLyKkp3VrFxRyUbtlexcWclG7dXMmPVNjZur2JP7RdbJWlJ8XTPSSW/Qyr5HdPI75BGfsfgfcc0stT6EIlqURkOAO7+MvBy2HXEsrg4o2NwXaKwS+Y+P1dX52zeVcW6skrWl1WwtqyCdWWVrC3bzeqtFRSv2MbOqpovfKdjehIFndIoyE2noFM6Bbnp9OmUTp+8dDKSo/ZnKRIz9K9QDltcnNE5M4XOmSmMyM9ptN7d2V5RzeqtFazetptVW3ezcssulm/exbSSLTw384uXk7pnp9CvcwaFnTPp3zmDwi4ZDOicSXaaWhsirUXhIBFnZuSkJZGTlsSRPbMbra/YU8vKrbtYsXkXS0t3sWTjTkpKy5n80Uoqqz8/XdU9O4VB3bIY3C2TQV2zGNwtiz656XpuRCQCFA4SutSkeAZ1zWJQ16wvLK+rc9aWVVCyqZxFG3eyYP0OFq7fybuLS6kJLpSnJMYxtHs2R/aon4b1zKZvXoYCQ+Qwmbf2E2ARUFRU5BoJLnZU1dRSsqmcBet3Mm/dduau3c7ctTuoqK4F6i+GH9E9m6N653BMrw4c07sDnTKSQ65aJPqY2Qx3L2pyncJB2oPaOmdpaTlz1tSHxSery5i/bjvVtfW/7z656RzdqwNFBR0Y2acjfXPTdautxDyFg8SkyupaPl27nRkrtzFj5TZmrtzGll17AOiSlcyYvp0Y068TY/vlkt/xMB4gEWmj9hcOuuYg7VZKYjzHFnTk2IKOQP1dU8s372L6sq1MW7qZ90o28/ysdQD0yEnl+MJcThyQx9j+uWSn6s4oiW1qOUjMcndKNpUzbekWpi3dzLSSLeysqiE+zjgqP4cTB+Rx4sA8juierQ4NpV3SaSWRZqiurWPW6jLeXVzKO4tLmbNmOwC5GUmMH9SFrw3pwrjCXFIS40OuVKRlKBxEDsHm8ir+s6SUNxZs4p1FpeysqiE1MZ4TBuTytSFdGT+oMx3Sk8IuU+SQKRxEDtOemjqmL9vC1PkbmTp/Ixt2VBIfZxzXP5ezhnXj1KFddZ1C2hyFg0gLcnfmrt3By3PX8+KcdazeWkFSfBwnDMjl7OHdOXlwF9LVP5S0AQoHkQhxd2av2c6/Zq/jpTnr2bCjkpTEOE4b2pULivIZ07eTLmZL1FI4iLSCujqneOU2np+1ln/NXsfOyhp65KRy3jE9Of/onvTqpGcpJLooHERaWWV1La/O28A/ZqzhvZLNuMOoPh25ZFQvTj+im4ZllaigcBAJ0bqyCp6buYa/z1jDyi27yc1I4qJje3HJqF50z0kNuzyJYQoHkShQV+e8u6SUv3ywkjcXbSLOjJMHd+aKMQWM7ddJfT1Jq1P3GSJRIC7OOGlgZ04a2JnVW3fz5Icreebj1bw6byMDu2Qy8YS+nD28u045SVRQy0EkRJXVtfxr9joe/c9yFm3cSbfsFK4e14eLRvbScKkScTqtJBLl3J23F5fyh3eWMn3ZVrJSErh8TG+uHFtA58yUsMuTdkrhINKGfLJqG5PeXcYr8zaQFB/H5aN7c/1J/cjVgEXSwhQOIm3QstJyHnyrhOc/WUtyQjxXjO3NdSf0o6P6c5IWonAQacOWlpbzwBtLeGH2OtIS47nquAKuPb4vOWkKCTk8CgeRdmDJxp3c/8YSXvp0PRlJCdz41f5cNbZAXYjLIVM4iLQjizbs5K5XF/L6gk30yEnlx6cP4uxh3fSchBy0/YWDbqgWaWMGds3k0SuPZfI1o8hOTeS7f/2Ebzw8jRkrt4VdmrQjCgeRNmps/1z+9Z1x3Hn+MNZuq+C8h6dx41MzWVtWEXZp0g4oHETasPg445tF+bz9o5O4eXwhbyzcyMm/fYdJ7y6lurYu7PKkDVM4iLQDaUkJfP9rA5j6/RM5rn8nfvXyQs5+4D1mrNwadmnSRikcRNqR/I5pPHrlsUy6/Bh2VFRz3sMfcOuzc9i2a0/YpUkbE0o4mNkFZjbPzOrMrOhL624zsxIzW2Rmp4ZRn0hbd8rQrkz9wYlMPKEvf5+xhvH3vMMLs9bSHu5OlNYRVsthLvAN4N2GC81sCHARMBQ4DXjIzHQTt8ghSE9O4PYzBvPid8bRq2MaNz89i+ufnMHm8qqwS5M2IJRwcPcF7r6oiVUTgKfdvcrdlwMlwMjWrU6kfRncLYtnvz2W204fxFuLSjnl3nd5ac76sMuSKBdt1xx6AKsbzK8JljViZhPNrNjMiktLS1ulOJG2Kj7OuO7Efrz0nXHkd0jlxskzuWnyTLbqWoTsQ8TCwcxeN7O5TUwTWmL77j7J3YvcvSgvL68lNinS7hV2yeTZb4/lR6cO5NV5Gzjl3neYOn9j2GVJFIrYaCLufvIhfG0tkN9gvmewTERaSEJ8HDd+pT/jB3fmB3+bzbVPFHPV2AJuPX2Q+mmSz0TbaaUpwEVmlmxmfYBC4KOQaxJplwZ1zeKfN47l6nF9+PO0FXzjoWksKy0PuyyJEmHdynquma0BxgAvmdmrAO4+D3gGmA+8Atzo7rVh1CgSC5IT4vmvs4bw2JVFrNtewVkPvMdzM9eEXZZEAfXKKiIArN9ewc1/ncVHK7Zy3tE9+Z8JQ0nXONbtmnplFZED6padyuRrR/Hd8YU898kaznnwPZbqNFPMUjiIyGcS4uP4wdcG8NTVoyjbXc3XH3yfNxbobqZYpHAQkUbG9s9lynfG0atTGtc8UcwDbyyhrq7tn4KW5lM4iEiTeuSk8o/rxzJheHd+O3UxNzw1k/KqmrDLklaicBCRfUpNiufeC0fw0zMH89r8DXzjofdZuWVX2GVJK1A4iMh+mRnXHN+XJ741ik07qzjnwfeZvmxL2GVJhCkcRKRZxhXmMuXGceRlJnPFYx/xwix1XtCeKRxEpNl6dUrj2evHclSvHG5+ehYPvV2iMSLaKYWDiByU7LREnrh6JOcM786dryziJ8/PpUbjVbc7evxRRA5ackI89104gh4dUnn47aWsL6vgwUuO1hPV7YhaDiJySOLijB+fNohffv0I3llcyoWTPmDTzsqwy5IWonAQkcNy2ejePHJFEUs37eLCP0xnXVlF2CVJC1A4iMhhGz+4C09cPZLNO6u44Pcf6FmIdkDhICIt4tiCjky+djS79tRwwe8/YMnGnWGXJIdB4SAiLebIntn8beIYHLhw0nTmrdsedklyiBQOItKiBnbN5JnrxpCSEMfFk6Yzc9W2sEuSQ6BwEJEW1yc3nWeuH0PH9CQue/RDdbfRBikcRCQienZI45nrxtAjJ5Vv/fljildsDbskOQgKBxGJmM5ZKTx17Si6ZqVw1Z8+ZtbqsrBLkmZSOIhIRHXOrA+IDumJXPHYh7pI3UYoHEQk4rplpzL5mtFkJCdw2aMfsmiDbnONdgoHEWkV+R3TmHztaBLj47j00Q9ZWloedkmyHwoHEWk1BbnpTL52NOBc8sh0PUkdxRQOItKq+nfO4MlrRlFVU8elj37Iph3qrC8aKRxEpNUN6prFE98aydZde7jyTx+zo7I67JLkSxQOIhKKYT1zePiyY1iycSfX/2UGVTW1YZckDSgcRCQ0Jw7I464LhjFt6RZ+8Mxs6uo05Gi00LBNIhKqc4/qSenOKn718kLyMpL577OHYGZhlxXzFA4iErqJJ/Rj044qHn1vOZ2zkrnhpP5hlxTzFA4iEhVuP2MwpeVV3PnKIvIykrmgKD/skmJaKNcczOwuM1toZnPM7J9mltNg3W1mVmJmi8zs1DDqE5HWFxdn3HX+cI4vzOW25z5lWsnmsEuKaWFdkJ4KHOHuw4DFwG0AZjYEuAgYCpwGPGRm8SHVKCKtLCkhjocuPZq+eelc/+QMlukp6tCEEg7u/pq71wSz04GewfsJwNPuXuXuy4ESYGQYNYpIODJTEnnsymNJiI/j6seLKdu9J+ySYlI03Mr6LeDfwfsewOoG69YEy0QkhuR3TGPS5cewdlsF335yJtW1dWGXFHMiFg5m9rqZzW1imtDgMz8BaoCnDmH7E82s2MyKS0tLW7J0EYkCRQUd+c35R/LBsi387IW5uOsZiNYUsbuV3P3k/a03s6uAs4Dx/vl/9bVAw1sUegbLmtr+JGASQFFRkX41Iu3QuUf1pGRTOb97ayn98jK45vi+YZcUM8K6W+k04BbgHHff3WDVFOAiM0s2sz5AIfBRGDWKSHT44dcGcvoRXbnj5QW8sWBj2OXEjLCuOTwIZAJTzWyWmf0ewN3nAc8A84FXgBvdXR2uiMSwuDjjt98cztDuWXz3r5+weKMGCmoN1h7O4xUVFXlxcXHYZYhIBG3YXslZD7xHZkoCL9x0HFkpiWGX1OaZ2Qx3L2pqXTTcrSQickBds1P43SVHsWrrbn6oTvoiTuEgIm3GqL6duP2MwUydv5GH31kadjntmsJBRNqUbx1XwDnDu3P3a4t4d7FuY48UhYOItClmxq/PO5KBXTL57tOfsHrr7gN/SQ6awkFE2py0pAR+f9kx1NY5335qBpXVuqmxpTUrHMzsTDO7xcx+tneKdGEiIvtTkJvOfReOYO7aHfz0eT1B3dIOGA7BMwgXAt8BDLgA6B3hukREDmj84C58d3wh/5ixhqc/Xn3gL0izNaflMNbdrwC2ufsvgDHAgMiWJSLSPN8bX8jxhbn8fMo8Fm7YEXY57UZzwqEieN1tZt2BaqBb5EoSEWm+uDjjnm+OICs1kZsmf8LuPTUH/pIcUHPC4cVgpLa7gJnACuCvkSxKRORg5GUmc9+FI1haWs7Pp8wLu5x2oTnhcKe7l7n7s9RfaxgE/DKyZYmIHJzj+udy01f680zxGp7/pMnOnOUgNCccPtj7JhihbXvDZSIi0eLm8YWMLOjIT/75Kcs37wq7nDZtn+FgZl3N7Bgg1cyOMrOjg+kkIK3VKhQRaaaE+Djuv3gEiQlx3DR5JlU1ev7hUO2v5XAqcDf1A+7cA/w2mL4P3B750kREDl637FTuPn8489bt4P+9vDDsctqsfY4E5+6PA4+b2XnB9QYRkTbh5CFd+NZxffjj+8sZ068Tpw7tGnZJbU5zrjm8b2aPmdm/AcxsiJldHeG6REQOy62nD+LIHtnc+uwcNu2oDLucNqc54fAn4FWgezC/GPhexCoSEWkBSQlx3HvhCCqqa7nl2TnqXuMgNSccct39GaAOwN1rAF3lEZGo179zBrefMZi3F5Xy5Ierwi6nTWlOOOwys06AA5jZaGB7RKsSEWkhl4/uzQkD8rjjpfksKy0Pu5w2oznh8ANgCtDPzN4HnqC+Ez4RkahnZtx1/jCSE+L5/t9mUV1bF3ZJbcIBw8HdZwInAmOB64Ch7j4n0oWJiLSULlkp/OrcI5m9Zju/e6sk7HLahOYO9jMSGA4cDVxsZldEriQRkZZ35rBunHtUDx54s4RZq8vCLifqNWc8h79Q/zDcOODYYCqKcF0iIi3uFxOG0jUrhe//bZZ6bz2AfT4E10ARMMR1H5iItHFZKYncfcFwLnl0Or96eQG//PqRYZcUtZpzWmkuoMcLRaRdGNOvE1cf14cnp69iWsnmsMuJWvvreO9fZjYFyAXmm9mrZjZl79R6JYqItKwfnjKQgk5p/Pi5OTq9tA/7azncTX2He2nA14FfUd/x3j1Al8iXJiISGalJ8dx5/nBWb63gzlcWhV1OVNpfx3vvAJhZ4t73e5lZaqQLExGJpJF9OnLlmN48/sEKzhzWjWMLOoZdUlTZ32mlb5vZp8BAM5vTYFoO6DkHEWnzbjltED1yUvnxP+ZQWa1egRra32mlycDZ1D8dfXaD6Rh3v6wVahMRiaj05AR+c94wlm3exb1TF4ddTlTZZzi4+3Z3X+HuF7v7ygbT1sPdqZn936AVMsvMXjOz7sFyM7P/NbOSYP3Rh7svEZH9Oa5/LhePzOeR/yzjk1Xbwi4najT3CemWdpe7D3P3EcCLwM+C5acDhcE0EXg4pPpEJIbcdsZgumSlcMs/5mho0UAo4eDuOxrMphP0+ApMAJ7wetOBHDPr1uoFikhMyUpJ5FfnHsmSTeU8+Kb6XoLwWg6Y2R1mthq4lM9bDj2A1Q0+tiZYJiISUV8Z1JlvHN2Dh95eyoL1Ow78hXYuYuFgZq+b2dwmpgkA7v4Td88HngJuOoTtTzSzYjMrLi0tbenyRSQG/deZQ8hOTeT2f35KXV1s9xgUsXBw95Pd/Ygmphe+9NGngPOC92uB/AbregbLmtr+JHcvcveivLy8lj8AEYk5HdKT+OmZg/lkVRmTP4rtkeNCOa1kZoUNZicAC4P3U4ArgruWRgPb3X19qxcoIjHr3KN6MLZfJ37zykI27awMu5zQhHXN4dfBKaY5wCnAzcHyl4FlQAnwCHBDSPWJSIwyM3759SOoqq7j/764IOxyQtOcLrtbnLuft4/lDtzYyuWIiHxB37wMbvxKf+59fTHnH9OTEwfE3qnr0O5WEhGJZtef1Je+een89PlPqdgTe88+KBxERJqQnBDPHV8/ktVbK3jgzSVhl9PqFA4iIvswpl8nzj+mJ5PeXcaiDTvDLqdVKRxERPbj9jMGk5mSwE9i7NkHhYOIyH50TE/i9jMGU7xyG3+fsfrAX2gnFA4iIgdw/jE9KerdgTtfWcT23dVhl9MqFA4iIgdgZvxiwlC27d7Dva/HxrgPCgcRkWYY2j2bS0f15okPVsREx3wKBxGRZvrhKQPITk3kv1+YR/0zu+2XwkFEpJly0pK45bRBfLRiK1Nmrwu7nIhSOIiIHIQLi/IZ3jObO15aQHlVTdjlRIzCQUTkIMTFGb+YcASbdlbxwBvt98lphYOIyEEakZ/DhUX5PPbecko2tc8npxUOIiKH4JbTBpKWFM/Pp8xvlxenFQ4iIoegU0YyPzxlIO+VbOaVuRvCLqfFKRxERA7RpaN6MahrJr/69wIqq9tXt94KBxGRQ5QQH8dPzxzC6q0V/HnairDLaVEKBxGRwzCuMJfxgzrz4JslbC6vCrucFqNwEBE5TLefOZjK6lrumdp++l1SOIiIHKZ+eRlcNro3T3+0ioUb2ke/SwoHEZEW8L2TC8lMSeSXLy5oF7e2KhxERFpATloS3zu5kPdKNvPmwk1hl3PYFA4iIi3kstG96ZuXzh0vLaC6ti7scg6LwkFEpIUkxsfxkzMGs2zzLv7ywcqwyzksCgcRkRb01UGdOb4wl/vfWELZ7j1hl3PIFA4iIi3IzPjpmUPYWVnNfa+33V5bFQ4iIi1sYNdMLjw2n6c+XMmqLbvDLueQKBxERCLgeycPID7OuPu1RWGXckgUDiIiEdAlK4VrxvVlyux1fLpme9jlHDSFg4hIhEw8sS8d0hL5zSsLwy7loCkcREQiJCslke98tf7BuHcXl4ZdzkEJNRzM7Idm5maWG8ybmf2vmZWY2RwzOzrM+kREDtelo3vRs0Mqv/73Qurq2k63GqGFg5nlA6cAqxosPh0oDKaJwMMhlCYi0mKSE+L50akDmb9+B1Nmrwu7nGYLs+VwL3AL0DBKJwBPeL3pQI6ZdQulOhGRFnL2sO4M7Z7F3a8toqqmbYwYF0o4mNkEYK27z/7Sqh7A6gbza4JlTW1jopkVm1lxaWnbOpcnIrElLs649fRBrNlWwZPTVx34C1EgYuFgZq+b2dwmpgnA7cDPDmf77j7J3YvcvSgvL69lihYRiZDjC/M4vjCXB99cwo7K6rDLOaCIhYO7n+zuR3x5ApYBfYDZZrYC6AnMNLOuwFogv8FmegbLRETavB+fNohtu6v5wztLwy7lgFr9tJK7f+rund29wN0LqD91dLS7bwCmAFcEdy2NBra7+/rWrlFEJBKO6JHNhBHdeey95WzaWRl2OfsVbc85vEx9y6IEeAS4IdxyRERa1vdPHkB1rfPQW9Hdegg9HIIWxObgvbv7je7ez92PdPfisOsTEWlJBbnpXHBMTyZ/uIq1ZRVhl7NPoYeDiEis+c74QgAeeCN6u/RWOIiItLIeOalcMqoXf5+xhhWbd4VdTpMUDiIiIbjhK/1IjDfue31x2KU0SeEgIhKCzpkpXDW2Dy/MXsfijTvDLqcRhYOISEiuO6EvGUkJ3PNa9LUeFA4iIiHpkJ7E1cf34ZV5G6JuQCCFg4hIiK4e14ectER+OzW6hhNVOIiIhCgzJZHrT+zH24tKKV6xNexyPqNwEBEJ2ZVjCsjNSOauVxfhHh0DAikcRERClpoUz01f6ceHy7fywdItYZcDKBxERKLCRSN70SUrmfui5KlphYOISBRISYzn+hP78VGUtB4UDiIiUeLikb3Iy0zm/jfCf+5B4SAiEiX2th6mL9vKh8vCbT0oHEREosilo3qRm5HM/SFfe1A4iIhEkfrWQ1+mLd3CxyE+96BwEBGJMpeO6k1uRhL3vx5e60HhICISZVKT4pl4Ql/eK9nMjJXhtB4UDiIiUeiy0b3plJ7EfSG1HhQOIiJRKC0pgWtP6Mt/lmxm5qptrb5/hYOISJS6fHRvOqaHc+1B4SAiEqXSkxO45vg+vLO4lFmry1p13woHEZEodsWYArJTE/ndWyWtul+Fg4hIFMtITuCqsQVMnb+RRRtab6xphYOISJS7amwBaUnxPPx267UeFA4iIlGuQ3oSl47qxZTZ61i1ZXer7FPhICLSBlx7fF8S4uJ4+J2lrbI/hYOISBvQOSuFC4p68uyMNWzYXhnx/SkcRETaiOtO6EetO4/+Z1nE96VwEBFpI3p1SuOc4d156sNVbNu1J6L7CiUczOznZrbWzGYF0xkN1t1mZiVmtsjMTg2jPhGRaPXtk/pRUV3Ln6atiOh+wmw53OvuI4LpZQAzGwJcBAwFTgMeMrP4EGsUEYkqA7pkcurQLvz5/eWUV9VEbD/RdlppAvC0u1e5+3KgBBgZck0iIlHlhpP6s6Oyhqemr4zYPsIMh5vMbI6Z/dHMOgTLegCrG3xmTbCsETObaGbFZlZcWloa6VpFRKLG8Pwcji/M5ZH/LKeyujYi+4hYOJjZ62Y2t4lpAvAw0A8YAawHfnuw23f3Se5e5O5FeXl5LVy9iEh0u+Gk/mwur+LvxasP/OFDkBCRrQLufnJzPmdmjwAvBrNrgfwGq3sGy0REpIHRfTtyzvDu5KQlRWT7EQuH/TGzbu6+Ppg9F5gbvJ8CTDaze4DuQCHwUQgliohENTPjfy8+KmLbDyUcgDvNbATgwArgOgB3n2dmzwDzgRrgRnePzAk1ERHZp1DCwd0v38+6O4A7WrEcERH5kmi7lVVERKKAwkFERBpROIiISCMKBxERaUThICIijSgcRESkEXP3sGs4bGZWChyoB6pcYHMrlBNtdNyxJ1aPXcd98Hq7e5P9D7WLcGgOMyt296Kw62htOu7YE6vHruNuWTqtJCIijSgcRESkkVgKh0lhFxASHXfsidVj13G3oJi55iAiIs0XSy0HERFpJoWDiIg0EhPhYGanmdkiMysxs1vDridSgvG4N5nZ3AbLOprZVDNbErx22N822iIzyzezt8xsvpnNM7Obg+Xt+tjNLMXMPjKz2cFx/yJY3sfMPgx+738zs8gMFRYyM4s3s0/M7MVgvt0ft5mtMLNPzWyWmRUHyyLyO2/34WBm8cDvgNOBIcDFZjYk3Koi5s/AaV9adivwhrsXAm8E8+1NDfBDdx8CjAZuDP4bt/djrwK+6u7DqR+P/TQzGw38BrjX3fsD24CrQ6wxkm4GFjSYj5Xj/oq7j2jwbENEfuftPhyAkY18XRsAAASFSURBVECJuy9z9z3A08CEkGuKCHd/F9j6pcUTgMeD948DX2/VolqBu69395nB+53U/w+jB+382L1eeTCbGEwOfBX4R7C83R03gJn1BM4EHg3mjRg47n2IyO88FsKhB7C6wfyaYFms6NJgvO4NQJcwi4k0MysAjgI+JAaOPTi1MgvYBEwFlgJl7l4TfKS9/t7vA24B6oL5TsTGcTvwmpnNMLOJwbKI/M7DGkNaQuDubmbt9t5lM8sAngW+5+476v+YrNdejz0YY32EmeUA/wQGhVxSxJnZWcAmd59hZieFXU8rG+fua82sMzDVzBY2XNmSv/NYaDmsBfIbzPcMlsWKjWbWDSB43RRyPRFhZonUB8NT7v5csDgmjh3A3cuAt4AxQI6Z7f3Drz3+3o8DzjGzFdSfJv4qcD/t/7hx97XB6ybq/xgYSYR+57EQDh8DhcGdDEnARcCUkGtqTVOAK4P3VwIvhFhLRATnmx8DFrj7PQ1WtetjN7O8oMWAmaUCX6P+estbwPnBx9rdcbv7be7e090LqP/3/Ka7X0o7P24zSzezzL3vgVOAuUTodx4TT0ib2RnUn6OMB/7o7neEXFJEmNlfgZOo78J3I/DfwPPAM0Av6rs1/6a7f/midZtmZuOA/wCf8vk56Nupv+7Qbo/dzIZRfwEynvo/9J5x9/8xs77U/0XdEfgEuMzdq8KrNHKC00r/x93Pau/HHRzfP4PZBGCyu99hZp2IwO88JsJBREQOTiycVhIRkYOkcBARkUYUDiIi0ojCQUREGlE4iIhIIwoHkSaYWY6Z3bCf9alm9k7QseO+PnP7YdZwt5l99XC2IXKodCurSBOCPppedPcj9rH+RiDB3e/fzzbK3T3jMGroDTzi7qcc6jZEDpVaDiJN+zXQL+g3/64m1l9K8CSqmXUzs3eDz841s+PN7NdAarDsqeBzlwXjL8wysz/sbXWYWbmZ3RuMyfCGmeUBuPtKoJOZdW2VIxZpQOEg0rRbgaVBv/k/argi6Ialr7uvCBZdArzq7iOA4cAsd78VqAi+f6mZDQYuBI4LPldLfcAApAPF7j4UeIf6J9v3mkl9X0IirUq9soocvFygrMH8x8Afg87/nnf3WU18ZzxwDPBx0FtsKp93kFYH/C14/yTwXIPvbQK6t1zpIs2jloPIwasAUvbOBIMsnUB9L6B/NrMrmviOAY8HLYkR7j7Q3X++j+03vBCYEuxPpFUpHESathPIbGqFu28D4s0sBT67cLzR3R+hfmSyo4OPVgetCagfvvH8oB/+veP+9g7WxfF5b6KXAO812N0A6nveFGlVCgeRJrj7FuD94AJzUxekXwPGBe9PAmab2SfUX1fYewfTJGCOmT3l7vOBn1I/itcc6kdt6xZ8bhcw0szmUj82wf/AZ2NU9AeKW/r4RA5Et7KKHAIzOxr4vrtf3gLbavKWVzM7Fzja3f/rcPchcrDUchA5BO4+E3hrfw/BtYAE4LcR3L7IPqnlICIijajlICIijSgcRESkEYWDiIg0onAQEZFGFA4iItLI/wcE7Z3TwWnNagAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2421c7",
      "metadata": {
        "id": "1d2421c7"
      },
      "source": [
        "### Soft Actor-critic (SAC)\n",
        "\n",
        "<div id=\"sac\"></div>\n",
        "\n",
        "Soft Actor-Critic is really close to DDPG, except that it learn using a regularised entropy on actions policy.\n",
        "\n",
        "#### Shanon entropy\n",
        "\n",
        "Let X be a random variable with a law of density p(X) satisfying the normalization and positivity requirements, we define its entropy by\n",
        "$$-\\int_{X} p(x) log (p(x))$$\n",
        "\n",
        "It allows to quantify the disorder of a random variable. The entropy is maximal when X follows an uniform distribution, and minimal when p(X) is equal to zero everywhere except in one value, which is a Dirac distribution.\n",
        "\n",
        "#### Why entropy in SAC?\n",
        "\n",
        "In SAC algorithm, we not only consider the critic evaluation inside the actor loss, but also it's decision entropy.\n",
        "The loss of the agent should be high when the entropy is low, because it will invite him to explore more.\n",
        "The mode the entropy of the agent is important, the more it will explore.\n",
        "\n",
        "More, SAC use entropy to estimate next_states value. The more the entropy of the target actor is important while choosing a', the more s' will be condidered as highly valuable.\n",
        "To illustrate the intiution behing this, let's imagine an environment with a discrete state space, in a sigle line: \n",
        "\n",
        "\n",
        "|    |    |    |\n",
        "|:--:|:--:|:--:|\n",
        "| +1 | A  | +1 |\n",
        "\n",
        "In the exemple above, the agent (A) will recieve a reward of +1 at the next round, whatever the next action he will chose. Because the actor loss make him maximise critig grade and entropy, he will improve entropy when the critig grage cannot be improved anymore. In this case, the critic value will everytime be the same, whatever the action taken by the agent. For this reason, the action entropy will be maximize, so the agent will have the same probability to go in any directions.\n",
        "\n",
        "This fact bring a side effect. Because states are consider good when the entropy of the action taken from them is high, the agent will maintain itself in states where he will be sake whatever the next taken action. This behavior is also present in DDPG but is strengten here but the entropy maximisation, making our agent gain in stability (you will observe the gap in performances standard deviation latter).\n",
        "\n",
        "\n",
        "|    |    |    |\n",
        "|:--:|:--:|:--:|\n",
        "| -1 | A  | +1 |\n",
        "\n",
        "In this new case, the entropy of the action will be low because the actor loss will be lower if the critic evaluation is high, so he should choose deterministic action that leads to states with the higher value.\n",
        "\n",
        "In other words, we can say that SAC will perform a better exploration/exploitation trade-off, by takin both the advantage of a high exploration, and the advantage of a high exploitation. He will maximize exploration by mawimising entropy, but will not suffer from the incovenient of high exploration that is fall in bad state and never enstrenght the optimal trajectory the the best rewards, because he will fly away from bad rewarding states.\n",
        "\n",
        "\n",
        "#### How to use entropy in SAC?\n",
        "\n",
        "To compute an entropy, we need the actor to give use a random distribution. The most common way to archieve that, is to make the actor with 2 * nb_action outputs neurons, giving actions means and standard deviations.\n",
        "\n",
        "Because the standart deviation, it is common to make the actor return the log(std), and then get the real std with $e^{log_std}$. But we can also put the std output inside a relu().\n",
        "\n",
        "To get the action, we can sample actions from the means and standard deviations we got. Doing this will prevent us latter to retro-propagate the gradient when we will train our actor, so we should do a reparametrization trick when we want to compute actions for actor training (more explanation in the code to fill bellow).\n",
        "Sample with reparametrisation is equivalent to compute $a = \\mu + \\mathcal{N}(0,\\,1)\\ * sigma$.\n",
        "\n",
        "In general, for a given a', we maximise entropy my minimising $log (p(x))$. Looking at the entropy function, we can understand that $log(p(x))$ have a higher impact than p(x) only, because p(x) is between 0 and 1, and $|log(p(x))|$ is generally extremly high because p(x) is generally close to 0.\n",
        "\n",
        "Because our environment action space is bounded, but our normal distribution is not, we can put the actions we got inside tanh function to get some between -1 and 1, and then scale it to our environment action space. Because we bould our actions, we should do a process over our log prob as follow:\n",
        "\n",
        "        log_probs = actions_distribution.log_prob(actions)\n",
        "        log_probs -= torch.log(1 - action.pow(2) + self.min_std)\n",
        "        log_probs = log_probs.sum(dim=-1)\n",
        "        \n",
        "We are not going deeper about these mathematicals details, but if you are interested, the explanation if this is in [SAC paper](https://arxiv.org/pdf/1812.05905.pdf), appendic C (bottom of page 16).\n",
        "\n",
        "Now you should have any informations to complete the code bellow for SAC implementation! To gain some time, wome code parts are already filled.\n",
        "\n",
        "NB: Algorithm hyperparameters are given in $__init__$ function, the new alpha hyper parameter is the ratio between Q value and entropy inside critic update:\n",
        "$$V(s') = TargetCritic(s', a') - alpha * LogProb$$ where $$a', LogProg = SampleAction(s')$$\n",
        "\n",
        "Note that SAC is very stable and converge well, so you don't neet to have low learning rate and you also can use a Sequential() model (like we did with DQN) without LayerNorm layers inside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35ef7f1",
      "metadata": {
        "id": "c35ef7f1"
      },
      "outputs": [],
      "source": [
        "class SACAgent(Agent):\n",
        "    def __init__(self, state_space, action_space, device, actor_lr=0.001, critic_lr=0.001, gamma=0.98, \n",
        "                 max_size=10000, tau=0.005, layer1_size=128, layer2_size=128, batch_size=128, alpha=0.9):\n",
        "        super().__init__(state_space, action_space, device, \"SAC\")\n",
        "        # TODO\n",
        "        pass\n",
        "    \n",
        "    def converge_to(self, model, other_model, tau=None):\n",
        "        \"\"\"\n",
        "        Make the first model weights converge to the second one with a ration of tau.\n",
        "        \"\"\"\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "        for self_param, other_param in zip(model.parameters(), other_model.parameters()):\n",
        "            self_param.data.copy_(\n",
        "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
        "            )\n",
        "\n",
        "    def sample_action(self, state, reparameterize=False, actor_network=None):\n",
        "        if actor_network is None:\n",
        "            actor_network = self.actor\n",
        "        # TODO\n",
        "        # 1. Sample an action from the given state using the given actor network. It can be the target or the \n",
        "        #    default actor depending on when this function is called\n",
        "        # 2. raparameterize is used to performed a reparametrisation trick if we want to keep the gradient \n",
        "        #    to retro-propagate it later. Use reparametrize=True when you call this function to train the actor.\n",
        "        #    if it is True, sample action using distribution.rsampl(), use sample() otherwise.\n",
        "        # 3. Compute the log_probability as explained in SAC description\n",
        "        # 4. Return both the actions taken and the log probabilities\n",
        "        pass\n",
        "\n",
        "    def action(self, state):\n",
        "        actions, _ = self.sample_action(state, reparameterize=False)\n",
        "        return actions.cpu().detach().numpy()\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) > self.batch_size:\n",
        "            states, actions, rewards, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "            # Training critic\n",
        "            # TODO\n",
        "\n",
        "            # Train actor\n",
        "            # TODO\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
        "        self.learn()\n",
        "        super().on_action_stop(action, new_state, reward, done)\n",
        "\n",
        "# Test our agent on LunarLanderContinuous-v2  (Don't work on Pendulum because of an unexpected bug)\n",
        "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
        "nb_seeds = 4\n",
        "sac_seeds_result = []\n",
        "ddpg_seeds_result = []\n",
        "for seed_id in range(nb_seeds):\n",
        "    \n",
        "    print()\n",
        "    print(\"###################\")\n",
        "    print()\n",
        "    print(\"      SEED \" + str(seed_id))\n",
        "    print()\n",
        "    print(\"###################\")\n",
        "    \n",
        "    print()\n",
        "    print(\" > Training SAC\")\n",
        "    print()\n",
        "    agent = SACAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
        "    sac_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
        "    \n",
        "    print()\n",
        "    print(\" > Training DDPG\")\n",
        "    print()\n",
        "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
        "    ddpg_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
        "\n",
        "    sac_means = np.mean(np.array(sac_seeds_result), axis=0)\n",
        "    sac_stds = np.std(np.array(sac_seeds_result), axis=0)\n",
        "\n",
        "    ddpg_means = np.mean(np.array(ddpg_seeds_result), axis=0)\n",
        "    ddpg_stds = np.std(np.array(ddpg_seeds_result), axis=0)\n",
        "\n",
        "    plt.cla()\n",
        "    plt.plot(sac_means, color=\"g\", label=\"sac\")\n",
        "    plt.fill_between([x for x in range(len(sac_means))], sac_means + sac_stds, sac_means - sac_stds, color=\"g\", alpha=0.2)\n",
        "    plt.plot(ddpg_means, color=\"r\", label=\"ddpg\")\n",
        "    plt.fill_between([x for x in range(len(ddpg_means))], ddpg_means + ddpg_stds, ddpg_means - ddpg_stds, color=\"b\", alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68894e45",
      "metadata": {
        "scrolled": true,
        "id": "68894e45"
      },
      "outputs": [],
      "source": [
        "# %load solutions/RL6_SAC.py\n",
        "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
        "\n",
        "from torch import optim\n",
        "import torch.nn.functional as f\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "class SACAgent(Agent):\n",
        "    def __init__(self, state_space, action_space, device, actor_lr=0.001, critic_lr=0.001, gamma=0.98,\n",
        "                 max_size=1000000, tau=0.005, layer1_size=128, layer2_size=128, batch_size=128, alpha=1):\n",
        "        super().__init__(state_space, action_space, device, \"SAC\")\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.replay_buffer = ReplayBuffer(max_size, self.device)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.actions_bounds_range = torch.tensor((action_space.high[0] - action_space.low[0]) / 2).to(self.device)\n",
        "        self.actions_bounds_mean = torch.tensor(mean((action_space.high[0], action_space.low[0]))).to(self.device)\n",
        "        self.min_std = 1e-6\n",
        "\n",
        "        self.actor = nn.Sequential(nn.Linear(self.state_size, layer1_size), nn.ReLU(),\n",
        "                                   nn.Linear(layer1_size, layer2_size), nn.ReLU(),\n",
        "                                   nn.Linear(layer2_size, 2 * self.nb_actions)).to(device=self.device).float()\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "        self.critic = nn.Sequential(nn.Linear(self.state_size + self.nb_actions, layer1_size), nn.ReLU(),\n",
        "                                    nn.Linear(layer1_size, layer2_size), nn.ReLU(),\n",
        "                                    nn.Linear(layer2_size, 1)).to(device=self.device).float()\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def converge_to(self, model, other_model, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "        for self_param, other_param in zip(model.parameters(), other_model.parameters()):\n",
        "            self_param.data.copy_(\n",
        "                self_param.data * (1.0 - tau) + other_param.data * tau\n",
        "            )\n",
        "\n",
        "    def sample_action(self, state, reparameterize=False, actor_network=None):\n",
        "        if actor_network is None:\n",
        "            actor_network = self.actor\n",
        "\n",
        "        # Forward\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).to(self.device)\n",
        "\n",
        "        actor_output = actor_network(state)\n",
        "\n",
        "        if len(state.shape) > 1:  # It's a batch\n",
        "            actions_means = actor_output[:, :self.nb_actions]\n",
        "            actions_stds = actor_output[:, self.nb_actions:]\n",
        "        else:\n",
        "            actions_means = actor_output[:self.nb_actions]\n",
        "            actions_stds = actor_output[self.nb_actions:]\n",
        "\n",
        "        actions_stds = torch.clamp(actions_stds, min=self.min_std, max=1)\n",
        "\n",
        "        actions_distribution = Normal(actions_means, actions_stds)\n",
        "\n",
        "        if reparameterize:\n",
        "            actions = actions_distribution.rsample()\n",
        "        else:\n",
        "            actions = actions_distribution.sample()\n",
        "        \n",
        "        action = torch.tanh(actions) * self.actions_bounds_range + self.actions_bounds_mean\n",
        "        log_probs = actions_distribution.log_prob(actions)\n",
        "        log_probs -= torch.log(1 - action.pow(2) + self.min_std)\n",
        "        log_probs = log_probs.sum(dim=-1)\n",
        "\n",
        "        return action, log_probs\n",
        "\n",
        "    def action(self, state):\n",
        "        actions, _ = self.sample_action(state, reparameterize=False)\n",
        "\n",
        "        return actions.cpu().detach().numpy()\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) > self.batch_size:\n",
        "            states, actions, rewards, next_states, done = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "            # Training critic\n",
        "            with torch.no_grad():\n",
        "                next_actions, next_log_probs = self.sample_action(next_states, actor_network=self.target_actor)\n",
        "                next_q_values = self.target_critic.forward(torch.cat((next_states, next_actions), -1)).view(-1)\n",
        "\n",
        "            q_hat = rewards + self.gamma * (1 - done) * (next_q_values - self.alpha * next_log_probs)\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            q_values = self.critic.forward(torch.cat((states, actions), 1)).view(-1)\n",
        "            critic_loss = f.mse_loss(q_values, q_hat)\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "            \n",
        "            self.converge_to(self.target_critic, self.critic)\n",
        "\n",
        "            # Train actor\n",
        "            actions, log_probs = self.sample_action(states, reparameterize=True)\n",
        "            log_probs = log_probs.view(-1)\n",
        "            critic_values = self.critic.forward(torch.cat((states, actions), -1)).view(-1)\n",
        "\n",
        "            actor_loss = log_probs - critic_values\n",
        "            actor_loss = torch.mean(actor_loss)\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward(retain_graph=True)\n",
        "            self.actor_optimizer.step()\n",
        "            \n",
        "            self.converge_to(self.target_actor, self.actor)\n",
        "\n",
        "    def on_action_stop(self, action, new_state, reward, done):\n",
        "        self.replay_buffer.append(self.last_state, action, reward, new_state, done)\n",
        "        self.learn()\n",
        "        super().on_action_stop(action, new_state, reward, done)\n",
        "\n",
        "# Test our agent on LunarLanderContinuous-v2\n",
        "environment = gym.make(\"LunarLanderContinuous-v2\")\n",
        "nb_seeds = 4\n",
        "sac_seeds_result = []\n",
        "ddpg_seeds_result = []\n",
        "for seed_id in range(nb_seeds):\n",
        "    \n",
        "    print()\n",
        "    print(\"###################\")\n",
        "    print()\n",
        "    print(\"      SEED \" + str(seed_id))\n",
        "    print()\n",
        "    print(\"###################\")\n",
        "    \n",
        "    print()\n",
        "    print(\" > Training SAC\")\n",
        "    print()\n",
        "    agent = SACAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
        "    sac_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
        "    \n",
        "    print()\n",
        "    print(\" > Training DDPG\")\n",
        "    print()\n",
        "    agent = DDPGAgent(environment.observation_space, environment.action_space, device=DEVICE)\n",
        "    ddpg_seeds_result.append(simulation(environment, agent, nb_episodes=80))\n",
        "\n",
        "    sac_means = np.mean(np.array(sac_seeds_result), axis=0)\n",
        "    sac_stds = np.std(np.array(sac_seeds_result), axis=0)\n",
        "\n",
        "    ddpg_means = np.mean(np.array(ddpg_seeds_result), axis=0)\n",
        "    ddpg_stds = np.std(np.array(ddpg_seeds_result), axis=0)\n",
        "\n",
        "    plt.cla()\n",
        "    plt.plot(sac_means, color=\"g\", label=\"sac\")\n",
        "    plt.fill_between([x for x in range(len(sac_means))], sac_means + sac_stds, sac_means - sac_stds, color=\"g\", alpha=0.2)\n",
        "    plt.plot(ddpg_means, color=\"r\", label=\"ddpg\")\n",
        "    plt.fill_between([x for x in range(len(ddpg_means))], ddpg_means + ddpg_stds, ddpg_means - ddpg_stds, color=\"b\", alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0935498c",
      "metadata": {
        "id": "0935498c"
      },
      "source": [
        "Now you can see that entropy regularisation improve so much DDPG performances.\n",
        "\n",
        "You don't need to remeber every performance improvement tricks we used in this notebook, but you should understand how DDPG and SAC works, and what are the differences between them, and between DQN and them."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}